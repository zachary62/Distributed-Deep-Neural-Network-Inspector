{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO support layers, units\n",
    "import ray\n",
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "import numpy as np\n",
    "# ray.shutdown()\n",
    "# ray.init(address=\"10.128.0.51:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Scanner(dni.access.Scan):\n",
    "    def __init__(self):\n",
    "        self.f = open('sqldata.txt', 'r')\n",
    "        self.f.seek(0)\n",
    "        self.nextline = self.f.readline()\n",
    "    def Next(self):\n",
    "        if not self.HasMore():\n",
    "            return \"\"\n",
    "        currentline = self.nextline\n",
    "        self.nextline = self.f.readline()\n",
    "        return self.postprocess(currentline)\n",
    "\n",
    "    def HasMore(self):\n",
    "        if self.nextline:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def postprocess(self,sttr):\n",
    "        return sttr.split(\"\\n\")[0]\n",
    "\n",
    "@ray.remote\n",
    "class ActivationExt(dni.model.Extractor):\n",
    "    def __init__(self, modelname, layer):\n",
    "        from keras.models import Model, load_model\n",
    "        newmodel = load_model(modelname)\n",
    "        newmodel._make_predict_function()\n",
    "        outputs = [newmodel.layers[l].output for l in layer]\n",
    "        self.model = Model(inputs = newmodel.input, outputs = outputs)\n",
    "    def extract(self,input,unit):\n",
    "        input = self.preprocess(input)\n",
    "        pred = self.model.predict(input)\n",
    "        if not isinstance(pred, list):\n",
    "            pred = [pred]\n",
    "        for i in range(len(pred)):\n",
    "            pred[i] = pred[i][...,unit]\n",
    "        return pred\n",
    "    def preprocess(self, input):\n",
    "        seq = input\n",
    "        char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    "         '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    "         '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    "         'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    "         'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    "         'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "        't': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}\n",
    "        pad_char='~'\n",
    "        w_size = 30\n",
    "        step_size = 5\n",
    "        n_test_tuples = 128\n",
    "        if w_size == -1:\n",
    "            w_size = max(len(S) for S in sequences)\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        steps = range(step_size, len(seq), step_size)\n",
    "        if len(steps) == 0:\n",
    "            steps = [len(seq)-1]\n",
    "        for i in steps:\n",
    "            end = i\n",
    "            start = max(0, i-w_size)\n",
    "            s = seq[start:end]\n",
    "            assert len(s) <= w_size\n",
    "            x = s.rjust(w_size, pad_char)\n",
    "            assert len(x) == w_size\n",
    "            X.append(x)\n",
    "            y.append(seq[i])\n",
    "            if len(X) > n_test_tuples:\n",
    "                break\n",
    "\n",
    "        test_from = X\n",
    "        X_test, char2int = dni.tool.TwoDimEncoders.raw_to_bin_tensor(test_from, cust_char2int=char2int)\n",
    "        return X_test\n",
    "    \n",
    "def generate_parsetree(inputdata):\n",
    "    import warnings\n",
    "    import copy\n",
    "    import collections\n",
    "    import time, timeit\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import Tree\n",
    "    from nltk import PCFG,CFG\n",
    "    from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "    from nltk.probability import DictionaryProbDist\n",
    "    expr = inputdata\n",
    "    gram = None\n",
    "    with open('sql_full_XL.pcfg', 'r') as f:\n",
    "        pcfg_string = f.read()\n",
    "        from nltk import PCFG\n",
    "        gram = PCFG.fromstring(pcfg_string)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    ix = rule_index\n",
    "    len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "    counter = 0\n",
    "    term_index = {}\n",
    "    new_rules = []\n",
    "    for rule in gram.productions():\n",
    "        # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "        if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "            new_rules.append(rule)\n",
    "            continue\n",
    "        # Otherwise creates a new rule\n",
    "        new_rhs = []\n",
    "        for r in rule.rhs():\n",
    "            if is_nonterminal(r) :\n",
    "                new_rhs.append(r)\n",
    "            else:\n",
    "                if r not in term_index:\n",
    "                    new_left = Nonterminal('symb_'+str(r))\n",
    "                    prule = ProbabilisticProduction(new_left,\n",
    "                                                    [r],\n",
    "                                                    prob=1.0)\n",
    "                    term_index[r]=prule\n",
    "                new_rhs.append(new_left)\n",
    "        new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                           new_rhs,\n",
    "                                           prob=rule.prob())\n",
    "        new_rules.append(new_rule)\n",
    "    new_rules += term_index.values()\n",
    "    gram = PCFG(gram.start(), new_rules)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    old_grammar = gram\n",
    "    # BFS\n",
    "    arules = set()\n",
    "    visited = set()\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            arules.add(r)\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.add(symb)\n",
    "    # Creates new rules\n",
    "    arules = list(arules)\n",
    "    gram = PCFG(old_grammar.start(), arules)\n",
    "\n",
    "    symbols = set()\n",
    "    for rule in gram.productions():\n",
    "        symbols.add(rule.lhs())\n",
    "        for r in rule.rhs():\n",
    "            symbols.add(r)\n",
    "\n",
    "    \n",
    "    term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "    terminals = set(term_nodes)\n",
    "    L = max(len(w) for w in terminals)\n",
    "    def terminal(i, expr):\n",
    "        for wid in reversed(range(1, L+1)):\n",
    "            j = i + wid\n",
    "            if j > len(expr):\n",
    "                continue\n",
    "            w = expr[i:j]\n",
    "            if w in terminals:\n",
    "                return w\n",
    "        return None\n",
    "\n",
    "    tokens = []\n",
    "    index = []\n",
    "    i = 0\n",
    "    skipchars=['~']\n",
    "    while i < len(expr):\n",
    "        if not expr[i] in skipchars:\n",
    "            term = terminal(i, expr)\n",
    "            if term is None:\n",
    "                if not ignore_errors:\n",
    "                    raise ValueError('Could not match token', expr[i:])\n",
    "                else:\n",
    "                    print(\"null\")\n",
    "            tokens.append(term)\n",
    "            index.append((i,len(term)))\n",
    "            i += len(term)\n",
    "        else:\n",
    "            i += 1\n",
    "    parser = nltk.EarleyChartParser(gram)\n",
    "    parse = None\n",
    "    for p in parser.parse(tokens):\n",
    "        parse = p\n",
    "        break\n",
    "\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    # Gets the symbols BFS\n",
    "    visited = [gram.start()]\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.append(symb)\n",
    "\n",
    "    visited = list(reversed(visited))\n",
    "\n",
    "    # Alternative method for checking correctness\n",
    "    alt_visited = [S for S in symbols if is_nonterminal(S)]\n",
    "    assert set(visited) == set(alt_visited)\n",
    "\n",
    "    non_term_nodes = visited\n",
    "    tree = parse\n",
    "    n_tokens = len(tree.leaves())\n",
    "    nt_symbols = non_term_nodes\n",
    "\n",
    "    rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "    rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "    def visit(tree, offset):\n",
    "        if isinstance(tree, Tree):\n",
    "            n_tokens = 0\n",
    "            for subtree in tree:\n",
    "                n_tokens += visit(subtree, offset + n_tokens)\n",
    "            symb = tree.label()\n",
    "            j = rule2feat[symb]\n",
    "            rule_feats[offset:offset+n_tokens, j] = 1\n",
    "            return n_tokens\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    visit(tree, 0)\n",
    "    seq = expr\n",
    "    lex_index = index\n",
    "    w_feats = rule_feats\n",
    "\n",
    "\n",
    "    ch_len = len(seq)\n",
    "    w_len  = w_feats.shape[0]\n",
    "    n_feats = w_feats.shape[1]\n",
    "    ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "    for i_w in range(w_len):\n",
    "        f_w  = w_feats[i_w,...]\n",
    "        i_ch,len_ch = lex_index[i_w]\n",
    "        f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "        ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "    return [ch_feats,nt_symbols]\n",
    "\n",
    "# feature for \"where\" symbol in sql\n",
    "def extractfeature1(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[119])\n",
    "\n",
    "# feature for \"From\" symbol in sql\n",
    "def extractfeature2(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 20:17:57,138\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m 2019-11-11 20:18:00.118074: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m 2019-11-11 20:18:00.125559: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m 2019-11-11 20:18:00.125974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f48bdc34e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m 2019-11-11 20:18:00.126010: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m 2019-11-11 20:18:00.126320: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=3397)\u001b[0m \n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |         Score         |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  |  0.10767211048930919  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  |  0.13059536112213385  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  |  0.12975068389073943  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.13260201050686735  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  |  -0.08673624851483597 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.057015309358898154 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  |  0.15837701420158165  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  |  0.13850702136329607  |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "\u001b[2m\u001b[36m(pid=3355)\u001b[0m python3: ../nptl/pthread_mutex_lock.c:352: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != ESRCH || !robust' failed.\n"
     ]
    }
   ],
   "source": [
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= Scanner\n",
    "ActivationExt = ActivationExt\n",
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]]\n",
    "FeatureNames = [\"WHERE\",\"FROM\"]\n",
    "MetricName = [\"Correlation\"]\n",
    "\n",
    "dni.inspect(clusterid = \"10.128.0.51:6379\",\n",
    "            AccessMethod= Scanner,\n",
    "            ActivationExt = ActivationExt, Neuron = [[[0],[0,1,2,3]]], \n",
    "            Models = [\"track_history/models-03-2.78.hdf5\"],\n",
    "            FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]],\n",
    "            FeatureNames = [\"WHERE\",\"FROM\"],\n",
    "            MetricName = [\"Correlation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
