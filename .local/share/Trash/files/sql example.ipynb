{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO support layers, units\n",
    "import ray\n",
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "import numpy as np\n",
    "# ray.shutdown()\n",
    "# ray.init(address=\"10.128.0.51:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Scanner(dni.access.Scan):\n",
    "    def __init__(self):\n",
    "        self.f = open('sqldata.txt', 'r')\n",
    "        self.f.seek(0)\n",
    "        self.nextline = self.f.readline()\n",
    "    def Next(self):\n",
    "        if not self.HasMore():\n",
    "            return \"\"\n",
    "        currentline = self.nextline\n",
    "        self.nextline = self.f.readline()\n",
    "        return self.postprocess(currentline)\n",
    "\n",
    "    def HasMore(self):\n",
    "        if self.nextline:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def postprocess(self,sttr):\n",
    "        return sttr.split(\"\\n\")[0]\n",
    "\n",
    "@ray.remote\n",
    "class ActivationExt(dni.model.Extractor):\n",
    "    def __init__(self, modelname, layer):\n",
    "        from keras.models import Model, load_model\n",
    "        newmodel = load_model(modelname)\n",
    "        newmodel._make_predict_function()\n",
    "        outputs = [newmodel.layers[l].output for l in layer]\n",
    "        self.model = Model(inputs = newmodel.input, outputs = outputs)\n",
    "    def extract(self,input,unit):\n",
    "        input = self.preprocess(input)\n",
    "        pred = self.model.predict(input)\n",
    "        if not isinstance(pred, list):\n",
    "            pred = [pred]\n",
    "        for i in range(len(pred)):\n",
    "            pred[i] = pred[i][...,unit]\n",
    "        return pred\n",
    "    def preprocess(self, input):\n",
    "        seq = input\n",
    "        char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    "         '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    "         '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    "         'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    "         'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    "         'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "        't': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}\n",
    "        pad_char='~'\n",
    "        w_size = 30\n",
    "        step_size = 5\n",
    "        n_test_tuples = 128\n",
    "        if w_size == -1:\n",
    "            w_size = max(len(S) for S in sequences)\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        steps = range(step_size, len(seq), step_size)\n",
    "        if len(steps) == 0:\n",
    "            steps = [len(seq)-1]\n",
    "        for i in steps:\n",
    "            end = i\n",
    "            start = max(0, i-w_size)\n",
    "            s = seq[start:end]\n",
    "            assert len(s) <= w_size\n",
    "            x = s.rjust(w_size, pad_char)\n",
    "            assert len(x) == w_size\n",
    "            X.append(x)\n",
    "            y.append(seq[i])\n",
    "            if len(X) > n_test_tuples:\n",
    "                break\n",
    "\n",
    "        test_from = X\n",
    "        X_test, char2int = dni.tool.TwoDimEncoders.raw_to_bin_tensor(test_from, cust_char2int=char2int)\n",
    "        return X_test\n",
    "    \n",
    "def generate_parsetree(inputdata):\n",
    "    import warnings\n",
    "    import copy\n",
    "    import collections\n",
    "    import time, timeit\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import Tree\n",
    "    from nltk import PCFG,CFG\n",
    "    from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "    from nltk.probability import DictionaryProbDist\n",
    "    expr = inputdata\n",
    "    gram = None\n",
    "    with open('sql_full_XL.pcfg', 'r') as f:\n",
    "        pcfg_string = f.read()\n",
    "        from nltk import PCFG\n",
    "        gram = PCFG.fromstring(pcfg_string)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    ix = rule_index\n",
    "    len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "    counter = 0\n",
    "    term_index = {}\n",
    "    new_rules = []\n",
    "    for rule in gram.productions():\n",
    "        # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "        if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "            new_rules.append(rule)\n",
    "            continue\n",
    "        # Otherwise creates a new rule\n",
    "        new_rhs = []\n",
    "        for r in rule.rhs():\n",
    "            if is_nonterminal(r) :\n",
    "                new_rhs.append(r)\n",
    "            else:\n",
    "                if r not in term_index:\n",
    "                    new_left = Nonterminal('symb_'+str(r))\n",
    "                    prule = ProbabilisticProduction(new_left,\n",
    "                                                    [r],\n",
    "                                                    prob=1.0)\n",
    "                    term_index[r]=prule\n",
    "                new_rhs.append(new_left)\n",
    "        new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                           new_rhs,\n",
    "                                           prob=rule.prob())\n",
    "        new_rules.append(new_rule)\n",
    "    new_rules += term_index.values()\n",
    "    gram = PCFG(gram.start(), new_rules)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    old_grammar = gram\n",
    "    # BFS\n",
    "    arules = set()\n",
    "    visited = set()\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            arules.add(r)\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.add(symb)\n",
    "    # Creates new rules\n",
    "    arules = list(arules)\n",
    "    gram = PCFG(old_grammar.start(), arules)\n",
    "\n",
    "    symbols = set()\n",
    "    for rule in gram.productions():\n",
    "        symbols.add(rule.lhs())\n",
    "        for r in rule.rhs():\n",
    "            symbols.add(r)\n",
    "\n",
    "    \n",
    "    term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "    terminals = set(term_nodes)\n",
    "    L = max(len(w) for w in terminals)\n",
    "    def terminal(i, expr):\n",
    "        for wid in reversed(range(1, L+1)):\n",
    "            j = i + wid\n",
    "            if j > len(expr):\n",
    "                continue\n",
    "            w = expr[i:j]\n",
    "            if w in terminals:\n",
    "                return w\n",
    "        return None\n",
    "\n",
    "    tokens = []\n",
    "    index = []\n",
    "    i = 0\n",
    "    skipchars=['~']\n",
    "    while i < len(expr):\n",
    "        if not expr[i] in skipchars:\n",
    "            term = terminal(i, expr)\n",
    "            if term is None:\n",
    "                if not ignore_errors:\n",
    "                    raise ValueError('Could not match token', expr[i:])\n",
    "                else:\n",
    "                    print(\"null\")\n",
    "            tokens.append(term)\n",
    "            index.append((i,len(term)))\n",
    "            i += len(term)\n",
    "        else:\n",
    "            i += 1\n",
    "    parser = nltk.EarleyChartParser(gram)\n",
    "    parse = None\n",
    "    for p in parser.parse(tokens):\n",
    "        parse = p\n",
    "        break\n",
    "\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    # Gets the symbols BFS\n",
    "    visited = [gram.start()]\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.append(symb)\n",
    "\n",
    "    visited = list(reversed(visited))\n",
    "\n",
    "    # Alternative method for checking correctness\n",
    "    alt_visited = [S for S in symbols if is_nonterminal(S)]\n",
    "    assert set(visited) == set(alt_visited)\n",
    "\n",
    "    non_term_nodes = visited\n",
    "    tree = parse\n",
    "    n_tokens = len(tree.leaves())\n",
    "    nt_symbols = non_term_nodes\n",
    "\n",
    "    rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "    rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "    def visit(tree, offset):\n",
    "        if isinstance(tree, Tree):\n",
    "            n_tokens = 0\n",
    "            for subtree in tree:\n",
    "                n_tokens += visit(subtree, offset + n_tokens)\n",
    "            symb = tree.label()\n",
    "            j = rule2feat[symb]\n",
    "            rule_feats[offset:offset+n_tokens, j] = 1\n",
    "            return n_tokens\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    visit(tree, 0)\n",
    "    seq = expr\n",
    "    lex_index = index\n",
    "    w_feats = rule_feats\n",
    "\n",
    "\n",
    "    ch_len = len(seq)\n",
    "    w_len  = w_feats.shape[0]\n",
    "    n_feats = w_feats.shape[1]\n",
    "    ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "    for i_w in range(w_len):\n",
    "        f_w  = w_feats[i_w,...]\n",
    "        i_ch,len_ch = lex_index[i_w]\n",
    "        f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "        ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "    return [ch_feats,nt_symbols]\n",
    "\n",
    "# feature for \"where\" symbol in sql\n",
    "def extractfeature1(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[119])\n",
    "\n",
    "# feature for \"From\" symbol in sql\n",
    "def extractfeature2(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[96])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= Scanner\n",
    "ActivationExt = ActivationExt\n",
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]]\n",
    "FeatureNames = [\"WHERE\",\"FROM\"]\n",
    "MetricName = [\"Correlation\"]\n",
    "\n",
    "dni.inspect(clusterid = \"10.128.0.51:6379\",\n",
    "            AccessMethod= Scanner,\n",
    "            ActivationExt = Act_extract, Neuron = [[[0],[0,1,2,3]]], \n",
    "            Models = [\"track_history/models-03-2.78.hdf5\"],\n",
    "            FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]],\n",
    "            FeatureNames = [\"WHERE\",\"FROM\"],\n",
    "            MetricName = [\"Correlation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 20:04:38,109\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-11-11 20:04:38,142\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(5d29414501000000), class name = Scanner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.698390: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ce7b88010 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707918: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m \n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |        Score         |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  | 0.11182267359630765  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  | 0.12861303059131324  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  | 0.14978813616253056  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.1254446026514693  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  | -0.09478801131318572 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.07576803250636789 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  |  0.1543156186363407  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  | 0.12079677432167589  |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "units = []\n",
    "ray.shutdown()\n",
    "ray.init(address=clusterid)\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])\n",
    "    \n",
    "maxlayer = 0\n",
    "maxunits = 0\n",
    "for i in range(len(Models)):\n",
    "    maxlayer = max(maxlayer,len(layers[i]))\n",
    "    maxunits = max(maxunits,len(units[i]))\n",
    "\n",
    "numfeature = 0\n",
    "for i in range(len(FeaturesFunctions)):\n",
    "    if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "        numfeature += len(FeaturesFunctions[i][0])\n",
    "    else:\n",
    "        numfeature += 1\n",
    "\n",
    "metrics = [None]*len(MetricName)\n",
    "for i in range(len(MetricName)):\n",
    "    if MetricName[i] == \"Correlation\":\n",
    "        metrics[i] = dni.metric.Batch_Incremental_Comprehensive_Correlation(len(Models),maxlayer,maxunits,numfeature)\n",
    "\n",
    "ModelActor = ActivationExt.remote(Models[0],layers[0])\n",
    "\n",
    "FeatActor = [None]*len(FeaturesFunctions)\n",
    "for i in range(len(FeaturesFunctions)):\n",
    "    # if has intermediate result\n",
    "    if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "        featureclass = ray.remote(dni.feature.CachedFeatureExtractor)\n",
    "        FeatActor[i] = featureclass.remote(FeaturesFunctions[i][0],FeaturesFunctions[i][1])\n",
    "    else:\n",
    "        FeatActor[i] = FeatureExtractor[i].remote(FeaturesFunctions[i],FeatureNames[i])\n",
    "\n",
    "ScannerActor = AccessMethod.remote()\n",
    "act = []\n",
    "feature = [[] for i in range(len(FeatureNames))]\n",
    "while ray.get(ScannerActor.HasMore.remote()):\n",
    "    data = ScannerActor.Next.remote()\n",
    "    Ext = ModelActor.extract.remote(data,units[0])\n",
    "    act.append(ray.get(Ext))\n",
    "    currf = 0\n",
    "    # for each group of feature functions\n",
    "    for i in range(len(FeaturesFunctions)):\n",
    "        if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "            FeatActor[i].setup.remote(data)\n",
    "            # for each feature functions\n",
    "            for j in range(len(FeaturesFunctions[i][0])):\n",
    "                FeatActor[i].extract.remote(data,1)\n",
    "                feature = ray.get(FeatActor[i].extract.remote(data,j))\n",
    "                # for each layer\n",
    "                for k in range(len(layers[0])):\n",
    "                    # for each units\n",
    "                    for l in range(len(units[i])):\n",
    "                        # for each metric\n",
    "                        for m in range(len(MetricName)):\n",
    "                            metrics[i].increment(feature,ray.get(Ext)[k][...,l],0,k,l,currf)\n",
    "                currf += 1\n",
    "        else:\n",
    "            feature[currf].append(ray.get(FeatActor[i].extract.remote(data)))\n",
    "            currf += 1\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "t = PrettyTable(['Model','Metric','Feature','Neuron','Score'])\n",
    "\n",
    "#for each model\n",
    "for i in range(len(Models)):\n",
    "    # for each layer\n",
    "    for j in range(len(layers[i])):\n",
    "        # for each unit\n",
    "        for k in range(len(units[i])):\n",
    "            # for each feature\n",
    "            for l in range(numfeature):\n",
    "                t.add_row([Models[i],MetricName[j],FeatureNames[l],\"(\"+str(layers[i][j])+\",\"+str(units[i][k])+\")\",metrics[i].extract(0,j,k,l)])\n",
    "                \n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:37.846780: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353405: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c7c4131790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353437: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.354687: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(CachedFeatureExtractor, c53598d002000000)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = []\n",
    "#for each model\n",
    "for i in range(len(Models)):\n",
    "    metric_temp1 = []\n",
    "    # for each layer\n",
    "    for j in range(len(layers[i])):\n",
    "        # for each unit\n",
    "        for k in range(len(units[i])):\n",
    "            # for each feature\n",
    "            for l in range(len(FeatureNames)):\n",
    "                \n",
    "                \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30, 4)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each load of input data, each layer, get a number of activation, the last dimension is unit\n",
    "act[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each feature, each load of input data, get a number of features\n",
    "feature[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |         Score         |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  |  0.02771628520146099  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  |  0.11961696236126476  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  |  0.05012749261812874  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.12826532216877126  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  |  -0.04503853262076593 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.038613080219511146 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  |  0.059537094383411385 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  |  0.13883371936802053  |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scanner.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(s.HasMore.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SELECT *, ix.* FROM )  SELECT pqohm.nl65p  AS bpxhg FROM yc11 WHERE p65.bn7<oo  AND ..false..<>.fj. ORDER BY ff DESC ) AS r08z union  SELECT .1875. * t8a5gu, false, wtl.* FROM vz AS rk4j6.* q36 AS w2orpr INNER JOIN l5  ON q3<=to4uao.tc WHERE f9c7kr<=azf8x GROUP BY  in7kmt$'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines  = []\n",
    "while ray.get(s.HasMore.remote()):\n",
    "    lines.append(s.Next.remote())\n",
    "ray.get(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = ray.get(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 171)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate = generate_parsetree(ray.get(lines[1]))\n",
    "intermediate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in enumerate(intermediate[1]):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 30)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = extractfeature1(intermediate,ray.get(lines[1]))\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureext = ray.remote(dni.feature.CachedFeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureext = featureext.remote([extractfeature1,extractfeature2],generate_parsetree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(ea7106654faf4b49c2340100000000c001000000)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureext.setup.remote(ray.get(lines[0]))\n",
    "ray.get(featureext.extract.remote(lines[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-260-54a76c4220d1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-260-54a76c4220d1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a = 1,b = 2\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= Scanner\n",
    "ActivationExt = ActivationExt\n",
    "Models = [\"Models/boolean.h5\"]\n",
    "FeaturesFunctions = features \n",
    "FeatureNames = feature_names\n",
    "FeatureExtractor = [F_Extractor]*6\n",
    "MetricExtractor = [corr_metric] MetricName = Metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = []\n",
    "units = []\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-10 20:16:22,983\tWARNING worker.py:1619 -- The actor or task with ID ffffffffffffd1cda02401000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {object_store_memory: 2.832031 GiB}, {CPU: 4.000000}, {memory: 8.251953 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.174985: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.182514: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.182985: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b48256a730 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.183017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.183389: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 30, 4)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelActor = ActivationExt.remote(Models[0],layers[0])\n",
    "act1 = ray.get(ModelActor.extract.remote(lines[1],units[0]))\n",
    "act1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
