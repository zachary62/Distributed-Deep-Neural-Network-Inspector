{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-25 20:58:37,937\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.128.0.51',\n",
       " 'object_store_address': '/tmp/ray/session_2019-11-24_05-06-15_858496_1204/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-11-24_05-06-15_858496_1204/sockets/raylet',\n",
       " 'redis_address': '10.128.0.51:6379',\n",
       " 'session_dir': '/tmp/ray/session_2019-11-24_05-06-15_858496_1204',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO support layers, units\n",
    "import ray\n",
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "import numpy as np\n",
    "ray.shutdown()\n",
    "ray.init(address=\"10.128.0.51:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationExt(dni.model.LocalActivationExt):\n",
    "    def preprocess(self, input):\n",
    "        seq = input\n",
    "        char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    "         '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    "         '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    "         'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    "         'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    "         'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "        't': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}\n",
    "        pad_char='~'\n",
    "        w_size = 30\n",
    "        step_size = 5\n",
    "        n_test_tuples = 128\n",
    "        if w_size == -1:\n",
    "            w_size = max(len(S) for S in sequences)\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        steps = range(step_size, len(seq), step_size)\n",
    "        if len(steps) == 0:\n",
    "            steps = [len(seq)-1]\n",
    "        for i in steps:\n",
    "            end = i\n",
    "            start = max(0, i-w_size)\n",
    "            s = seq[start:end]\n",
    "            assert len(s) <= w_size\n",
    "            x = s.rjust(w_size, pad_char)\n",
    "            assert len(x) == w_size\n",
    "            X.append(x)\n",
    "            y.append(seq[i])\n",
    "            if len(X) > n_test_tuples:\n",
    "                break\n",
    "\n",
    "        test_from = X\n",
    "        X_test, char2int = dni.tool.TwoDimEncoders.raw_to_bin_tensor(test_from, cust_char2int=char2int)\n",
    "        return X_test\n",
    "    \n",
    "def generate_parsetree(inputdata):\n",
    "    import warnings\n",
    "    import copy\n",
    "    import collections\n",
    "    import time, timeit\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import Tree\n",
    "    from nltk import PCFG,CFG\n",
    "    from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "    from nltk.probability import DictionaryProbDist\n",
    "    expr = inputdata\n",
    "    gram = None\n",
    "    with open('sql_full_XL.pcfg', 'r') as f:\n",
    "        pcfg_string = f.read()\n",
    "        from nltk import PCFG\n",
    "        gram = PCFG.fromstring(pcfg_string)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    ix = rule_index\n",
    "    len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "    counter = 0\n",
    "    term_index = {}\n",
    "    new_rules = []\n",
    "    for rule in gram.productions():\n",
    "        # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "        if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "            new_rules.append(rule)\n",
    "            continue\n",
    "        # Otherwise creates a new rule\n",
    "        new_rhs = []\n",
    "        for r in rule.rhs():\n",
    "            if is_nonterminal(r) :\n",
    "                new_rhs.append(r)\n",
    "            else:\n",
    "                if r not in term_index:\n",
    "                    new_left = Nonterminal('symb_'+str(r))\n",
    "                    prule = ProbabilisticProduction(new_left,\n",
    "                                                    [r],\n",
    "                                                    prob=1.0)\n",
    "                    term_index[r]=prule\n",
    "                new_rhs.append(new_left)\n",
    "        new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                           new_rhs,\n",
    "                                           prob=rule.prob())\n",
    "        new_rules.append(new_rule)\n",
    "    new_rules += term_index.values()\n",
    "    gram = PCFG(gram.start(), new_rules)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    old_grammar = gram\n",
    "    # BFS\n",
    "    arules = set()\n",
    "    visited = set()\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            arules.add(r)\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.add(symb)\n",
    "    # Creates new rules\n",
    "    arules = list(arules)\n",
    "    gram = PCFG(old_grammar.start(), arules)\n",
    "\n",
    "    symbols = set()\n",
    "    for rule in gram.productions():\n",
    "        symbols.add(rule.lhs())\n",
    "        for r in rule.rhs():\n",
    "            symbols.add(r)\n",
    "\n",
    "    \n",
    "    term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "    terminals = set(term_nodes)\n",
    "    L = max(len(w) for w in terminals)\n",
    "    def terminal(i, expr):\n",
    "        for wid in reversed(range(1, L+1)):\n",
    "            j = i + wid\n",
    "            if j > len(expr):\n",
    "                continue\n",
    "            w = expr[i:j]\n",
    "            if w in terminals:\n",
    "                return w\n",
    "        return None\n",
    "\n",
    "    tokens = []\n",
    "    index = []\n",
    "    i = 0\n",
    "    skipchars=['~']\n",
    "    while i < len(expr):\n",
    "        if not expr[i] in skipchars:\n",
    "            term = terminal(i, expr)\n",
    "            if term is None:\n",
    "                if not ignore_errors:\n",
    "                    raise ValueError('Could not match token', expr[i:])\n",
    "                else:\n",
    "                    print(\"null\")\n",
    "            tokens.append(term)\n",
    "            index.append((i,len(term)))\n",
    "            i += len(term)\n",
    "        else:\n",
    "            i += 1\n",
    "    parser = nltk.EarleyChartParser(gram)\n",
    "    parse = None\n",
    "    for p in parser.parse(tokens):\n",
    "        parse = p\n",
    "        break\n",
    "\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    # Gets the symbols BFS\n",
    "    visited = [gram.start()]\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.append(symb)\n",
    "\n",
    "    visited = list(reversed(visited))\n",
    "\n",
    "    # Alternative method for checking correctness\n",
    "    alt_visited = [S for S in symbols if is_nonterminal(S)]\n",
    "    assert set(visited) == set(alt_visited)\n",
    "\n",
    "    non_term_nodes = visited\n",
    "    tree = parse\n",
    "    n_tokens = len(tree.leaves())\n",
    "    nt_symbols = non_term_nodes\n",
    "\n",
    "    rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "    rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "    def visit(tree, offset):\n",
    "        if isinstance(tree, Tree):\n",
    "            n_tokens = 0\n",
    "            for subtree in tree:\n",
    "                n_tokens += visit(subtree, offset + n_tokens)\n",
    "            symb = tree.label()\n",
    "            j = rule2feat[symb]\n",
    "            rule_feats[offset:offset+n_tokens, j] = 1\n",
    "            return n_tokens\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    visit(tree, 0)\n",
    "    seq = expr\n",
    "    lex_index = index\n",
    "    w_feats = rule_feats\n",
    "\n",
    "\n",
    "    ch_len = len(seq)\n",
    "    w_len  = w_feats.shape[0]\n",
    "    n_feats = w_feats.shape[1]\n",
    "    ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "    for i_w in range(w_len):\n",
    "        f_w  = w_feats[i_w,...]\n",
    "        i_ch,len_ch = lex_index[i_w]\n",
    "        f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "        ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "    return [ch_feats,nt_symbols]\n",
    "\n",
    "# feature for \"where\" symbol in sql\n",
    "def extractfeature1(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[119])\n",
    "\n",
    "# feature for \"From\" symbol in sql\n",
    "def extractfeature2(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[96])\n",
    "\n",
    "registry = dni.udf.UDFRegistry.registry()\n",
    "registry.add(generate_parsetree,\"parsing_tree\")\n",
    "registry.add(extractfeature2,\"from_symbol_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= [[dni.access.LocalScanner,['sqldata.txt']]]\n",
    "ActivationExt = ActivationExt\n",
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "FeaturesFunctions = [[[extractfeature1,\"from_symbol_feature\"],\"parsing_tree\"]]\n",
    "FeatureNames = [\"WHERE\",\"FROM\"]\n",
    "MetricName = [\"Correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[<function __main__.extractfeature1(intermiediate, inputdata)>,\n",
       "   'from_symbol_feature'],\n",
       "  'parsing_tree']]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "FeaturesFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = dni.udf.UDFRegistry.registry()\n",
    "for index1, elem1 in enumerate(FeaturesFunctions):\n",
    "    for index2, elem2 in enumerate(elem1):\n",
    "        if isinstance(elem2, list):\n",
    "            for index3, elem3 in enumerate(elem2):\n",
    "                if type(elem3) is str:\n",
    "                    FeaturesFunctions[index1][index2][index3] = registry[elem3]\n",
    "        else:\n",
    "            if type(elem2) is str:\n",
    "                FeaturesFunctions[index1][index2] = registry[elem2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[<function __main__.extractfeature1(intermiediate, inputdata)>,\n",
       "   <function __main__.extractfeature2(intermiediate, inputdata)>],\n",
       "  <function __main__.generate_parsetree(inputdata)>]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeaturesFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_parsetree(inputdata)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeaturesFunctions[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-25 22:20:48,858\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m 2019-11-25 22:20:54.125244: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m 2019-11-25 22:20:54.135132: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m 2019-11-25 22:20:54.135570: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5633c60230f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m 2019-11-25 22:20:54.135642: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m 2019-11-25 22:20:54.136032: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=3477)\u001b[0m \n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |        Score         |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  | 0.03373130416851632  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  | 0.11239269869466469  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  | 0.04085622624552211  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  | 0.11574334815141114  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  | -0.05180916966980314 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.08820863471677727 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  | 0.055867415874806335 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  |  0.1302720191246614  |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= [[dni.access.LocalScanner,'sqldata.txt']]\n",
    "ActivationExt = ActivationExt\n",
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "FeaturesFunctions = [[[extractfeature1,\"from_symbol_feature\"],\"parsing_tree\"]]\n",
    "FeatureNames = [\"WHERE\",\"FROM\"]\n",
    "MetricName = [\"Correlation\"]\n",
    "\n",
    "\n",
    "dni.inspect(clusterid = \"10.128.0.51:6379\",\n",
    "            AccessMethod= AccessMethod,\n",
    "            ActivationExt = ActivationExt, Neuron = [[[0],[0,1,2,3]]], \n",
    "            Models = [\"track_history/models-03-2.78.hdf5\"],\n",
    "            FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]],\n",
    "            FeatureNames = [\"WHERE\",\"FROM\"],\n",
    "            MetricName = [\"Correlation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-25 20:28:34,756\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-11-25 20:28:34,787\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(cb5a56e514000000), class name = PhysicalScanner.\n",
      "2019-11-25 20:28:34,814\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(6c3c451114000000), class name = PhysicalScanner.\n",
      "2019-11-25 20:28:34,853\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(3cc9d99c14000000), class name = PhysicalActivationExt.\n",
      "2019-11-25 20:28:34,854\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(b0fcf02d14000000), class name = PhysicalFeatureExt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m 2019-11-25 20:28:37.632477: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m 2019-11-25 20:28:37.641466: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m 2019-11-25 20:28:37.642125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5587a4d04a40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m 2019-11-25 20:28:37.642188: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m 2019-11-25 20:28:37.642731: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=1142)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= [[dni.access.LocalScanner,'sqldata.txt']]\n",
    "ActivationExt = ActivationExt\n",
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "FeaturesFunctions = [[[extractfeature1,extractfeature2],generate_parsetree]]\n",
    "FeatureNames = [\"WHERE\",\"FROM\"]\n",
    "MetricName = [\"Correlation\"]\n",
    "ray.shutdown()\n",
    "ray.init(address=clusterid)\n",
    "\n",
    "nummodel = len(Models)\n",
    "layers = []\n",
    "units = []\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])\n",
    "maxlayer = 0\n",
    "maxunits = 0\n",
    "for i in range(len(Models)):\n",
    "    maxlayer = max(maxlayer,len(layers[i]))\n",
    "    maxunits = max(maxunits,len(units[i]))\n",
    "numfeature = 0\n",
    "for i in range(len(FeaturesFunctions)):\n",
    "    if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "        numfeature += len(FeaturesFunctions[i][0])\n",
    "    else:\n",
    "        numfeature += 1\n",
    "file_name = AccessMethod[0][1]\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "phy = dni.access.build_physical_scanner(dni.access.LocalScanner)\n",
    "ps1 = phy.remote(file_name,batch_size)\n",
    "phy_f_act = dni.feature.build_physical_feature_ext(dni.feature.FeatureExtractor)\n",
    "fxt = phy_f_act.remote(ps1,FeaturesFunctions[0][1],FeaturesFunctions[0][0])\n",
    "ps2 = phy.remote(file_name,batch_size)\n",
    "phy_m_act = dni.model.build_physical_activation_ext(ActivationExt)\n",
    "m = phy_m_act.remote(ps2,Models[0],layers[0],units[0])\n",
    "corr_metric = dni.metric.IncrementalCorrelation(fxt,m,nummodel,maxlayer,maxunits,numfeature)\n",
    "# corr_metric.open_itr()\n",
    "# from prettytable import PrettyTable\n",
    "# t = PrettyTable(['Model','Metric','Feature','Neuron','Score'])\n",
    "# #for each model\n",
    "# for i in range(len(Models)):\n",
    "#     # for each layer\n",
    "#     for j in range(len(layers[i])):\n",
    "#         # for each unit\n",
    "#         for k in range(len(units[i])):\n",
    "#             # for each feature\n",
    "#             for l in range(2):\n",
    "#                 t.add_row([Models[i],MetricName[j],FeatureNames[l],\"(\"+str(layers[i][j])+\",\"+str(units[i][k])+\")\",corr_metric.extract(0,j,k,l)])               \n",
    "# print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(ps1.has_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0a99ad28a088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tables'"
     ]
    }
   ],
   "source": [
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feature' from '/home/zh2408/feature.py'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "importlib.reload(dni.feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = dni.access.build_physical_scanner(dni.access.LocalScanner)\n",
    "ps1 = phy.remote(file_name,batch_size)\n",
    "#dynamicaaly build physical activation extractor based on logical activation extractor\n",
    "def build_physical_activation_ext(LogicalActivationExt):\n",
    "#     @ray.remote\n",
    "    class PhysicalActivationExt(LogicalActivationExt):\n",
    "        def get_next(self):\n",
    "            if not self.has_next():\n",
    "                return None\n",
    "            input_table = ray.get(self.c.get_next.remote())\n",
    "            table = dni.tool.HighDimensionPartitionableTable()\n",
    "            # read the whole batch of inputs\n",
    "            for num, input in input_table.itr():\n",
    "                table.merge(dni.tool.activations_list_to_array(self.predict(input)))        \n",
    "            return table\n",
    "        def has_next(self):\n",
    "            return super().has_next()\n",
    "    return PhysicalActivationExt\n",
    "phy_m_act = build_physical_activation_ext(ActivationExt)\n",
    "m = phy_m_act(ps1,Models[0],layers[0],units[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = dni.access.build_physical_scanner(dni.access.LocalScanner)\n",
    "ps1 = phy.remote(file_name,batch_size)\n",
    "#dynamicaaly build physical activation extractor based on logical activation extractor\n",
    "def build_physical_feature_ext(LogicalFeatureExt):\n",
    "    class PhysicalFeatureExt(LogicalFeatureExt):\n",
    "        def get_next(self):\n",
    "            if not self.has_next():\n",
    "                return None\n",
    "            # implement same schema for table\n",
    "            # implement the partition operator\n",
    "            input_table = ray.get(self.c.get_next.remote())\n",
    "            table = dni.tool.HighDimensionPartitionableTable()\n",
    "            # read the whole batch of inputs\n",
    "            for num, input in input_table.itr():\n",
    "                table.merge(dni.tool.features_list_to_array(self.extract(input)))    \n",
    "            return table\n",
    "        def has_next(self):\n",
    "            return super().has_next()\n",
    "    return PhysicalFeatureExt\n",
    "phy_f_act = build_physical_feature_ext(dni.feature.FeatureExtractor)\n",
    "fxt = phy_f_act(ps1,FeaturesFunctions[0][1],FeaturesFunctions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = fxt.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 54, 30, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 1, 4, 1, 30)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.array(m), (1, 0, 3, 2))[..., np.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 2, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(np.array(k), (1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.table[0][0][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 1,\n",
       " 'input_num': 79,\n",
       " 'layer_num': 1,\n",
       " 'model_num': 1,\n",
       " 'shape': (30,),\n",
       " 'unit_num': 4}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 2,\n",
       " 'input_num': 79,\n",
       " 'layer_num': 1,\n",
       " 'model_num': 1,\n",
       " 'shape': (30,),\n",
       " 'unit_num': 1}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 0, 0, 0] (30,)\n",
      "[5, 0, 0, 1, 0] (30,)\n",
      "[5, 0, 0, 2, 0] (30,)\n",
      "[5, 0, 0, 3, 0] (30,)\n"
     ]
    }
   ],
   "source": [
    "for i,j in m.itr(5):\n",
    "    print(i,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activations_list_to_array(activations):\n",
    "    activation_array = []\n",
    "    for activation in np.transpose(np.array(activations), (1, 0, 3, 2))[..., np.newaxis,:]:\n",
    "        activation_array.append([activation])\n",
    "    return activation_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 1, 1, 4, 1, 30)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(activations_list_to_array(m)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "registry = dni.udf.UDFRegistry.registry()\n",
    "registry.add(np.mean,\"avg\")\n",
    "udf = registry[\"avg\"]\n",
    "udf([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry = dni.udf.UDFRegistry.registry()\n",
    "udf = registry[\"avg\"]\n",
    "udf([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseops.UnaryOp"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dni.baseops.UnaryOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Scanner(dni.access.Scan):\n",
    "    def __init__(self):\n",
    "        self.f = open('sqldata.txt', 'r')\n",
    "        self.f.seek(0)\n",
    "        self.nextline = self.f.readline()\n",
    "    def Next(self):\n",
    "        if not self.HasMore():\n",
    "            return \"\"\n",
    "        currentline = self.nextline\n",
    "        self.nextline = self.f.readline()\n",
    "        return self.postprocess(currentline)\n",
    "    def HasMore(self):\n",
    "        if self.nextline:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def postprocess(self,sttr):\n",
    "        return sttr.split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BroadCaster(dni.baseops.UnaryOp):\n",
    "    def get_next(self):\n",
    "        # get tables from child and merge all of them\n",
    "        if self.table == None:\n",
    "            table = dni.tool.HighDimensionPartitionableTable()\n",
    "            while ray.get(self.c.has_next.remote())\n",
    "                table.merge(ray.get(self.feature_child.get_next.remote()))\n",
    "            self.table = table\n",
    "        # return tables to parent\n",
    "        return self.table\n",
    "    def has_next(self):\n",
    "        return True\n",
    "\n",
    "class Partitioner(dni.baseops.UnaryOp):\n",
    "    def __init__(self, child, partition_number):\n",
    "        super().__init__(child)\n",
    "        self.partition_number = partition_number\n",
    "        self.stack_of_partitions = []\n",
    "    def get_next(self):\n",
    "        # get table from child and partition it\n",
    "        if len(self.stack_of_partitions) == 0:\n",
    "            self.stack_of_partitions = ray.get(self.c.get_next.remote()).partition(self.partition_number)\n",
    "        return self.stack_of_partitions.pop()\n",
    "    def has_next(self):\n",
    "        return ray.get(self.c.has_next.remote()) or len(self.stack_of_partitions) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = LocalScanner.remote('sqldata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT *, ix.* FROM )  SELECT pqohm.nl65p  AS bpxhg FROM yc11 WHERE p65.bn7<oo  AND ..false..<>.fj. ORDER BY ff DESC ) AS r08z union  SELECT .1875. * t8a5gu, false, wtl.* FROM vz AS rk4j6.* q36 AS w2orpr INNER JOIN l5  ON q3<=to4uao.tc WHERE f9c7kr<=azf8x GROUP BY  in7kmt$\n"
     ]
    }
   ],
   "source": [
    "if ray.get(c.have_next.remote()):\n",
    "    print(ray.get(c.get_next.remote()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "Models = [\"Models/boolean.h5\"]\n",
    "layers = []\n",
    "units = []\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19994)\u001b[0m python3: ../nptl/pthread_mutex_lock.c:352: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != ESRCH || !robust' failed.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m 2019-11-17 19:01:14.654785: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m 2019-11-17 19:01:14.661854: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m 2019-11-17 19:01:14.662173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5611c42bfb80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m 2019-11-17 19:01:14.662204: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m 2019-11-17 19:01:14.662493: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=20145)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "c = LocalScanner.remote('sqldata.txt')\n",
    "m = ActivationExt.remote(c,\"track_history/models-03-2.78.hdf5\",layers[0],units[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ray.get(m.have_next.remote())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ActivationTable at 0x7f20c1082e48>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ray.get(m.get_next.remote())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_shape': (30,), 'input_num': 54, 'layer_num': 1, 'unit_num': 4}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_stat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_per_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][...,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 108, 30)\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "# b = np.array([[9, 8, 7], [6, 5, 4]])\n",
    "table1 = t\n",
    "table2 = t\n",
    "layer_num = len(table1)\n",
    "unit_num = table1[0].shape[-1]\n",
    "\n",
    "new_table = []\n",
    "for i in range(layer_num):\n",
    "    activation_per_layer = []\n",
    "    for j in range(unit_num):\n",
    "        activation_per_layer.append(np.concatenate((table1[i][...,j], table2[i][...,j])))\n",
    "    print(np.array(activation_per_layer).shape)\n",
    "    new_table.append(np.transpose(np.array(activation_per_layer), (1, 2, 0)))  \n",
    "    \n",
    "# np.concatenate((t[0][...,0], t[0][...,0])).shape\n",
    "# activation_per_layer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = len(new_table)\n",
    "unit_num = new_table[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalScanner(Operator):\n",
    "    def __init__(self, filename):\n",
    "        self.f = open(filename, 'r')\n",
    "        self.f.seek(0)\n",
    "        self.nextline = self.f.readline()\n",
    "    def get_next(self):\n",
    "        if not self.have_next():\n",
    "            return None\n",
    "        currentline = self.nextline\n",
    "        self.nextline = self.f.readline()\n",
    "        return self.post_process(currentline)\n",
    "    def have_next(self):\n",
    "        if self.nextline:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def post_process(self,sttr):\n",
    "        return sttr.split(\"\\n\")[0]\n",
    "\n",
    "#dynamicaaly build physical scanner based on logical scanner\n",
    "def build_physical_scanner(LogicalScanner):\n",
    "    @ray.remote\n",
    "    class PhysicalScanner(LogicalScanner):\n",
    "        def __init__(self,file,batch):\n",
    "            super().__init__(file)\n",
    "            self.batch = batch  \n",
    "        def get_next(self):\n",
    "            batch_inputs = []\n",
    "            while self.have_next() and len(batch_inputs)<self.batch:\n",
    "                batch_inputs.append(super().get_next())\n",
    "            return InputTable(batch_inputs) \n",
    "        def have_next(self):\n",
    "            return super().have_next()\n",
    "    return PhysicalScanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = build_physical_scanner(LocalScanner)\n",
    "ps = phy.remote('sqldata.txt',2)\n",
    "asd = ray.get(ps.get_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = phy.remote('sqldata.txt',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = ray.get(ps.get_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_num': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = None\n",
    "for i,j in asd.itr():\n",
    "    sample_input = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SELECT mwfl.* ORDER BY v1d.rk ASC ) o3py$'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = t\n",
    "table2 = t\n",
    "layer_num = len(table1)\n",
    "unit_num = table1[0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30, 4)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 54, 30)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(t[0], (2, 0, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ActivationTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not initialized'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_shape': (30,), 'input_num': 216, 'layer_num': 1, 'unit_num': 4}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.add_table(t)\n",
    "table.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 30, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "class ActivationExt(Operator):\n",
    "    def __init__(self, accessmethod, modelname, layer,unit):\n",
    "        from keras.models import Model, load_model\n",
    "        newmodel = load_model(modelname)\n",
    "        newmodel._make_predict_function()\n",
    "        outputs = [newmodel.layers[l].output for l in layer]\n",
    "        self.model = Model(inputs = newmodel.input, outputs = outputs)\n",
    "        self.unit = unit\n",
    "        self.c = accessmethod\n",
    "    def get_next(self):\n",
    "        if not self.have_next():\n",
    "            return None\n",
    "        input = ray.get(self.c.get_next.remote())\n",
    "        pred = self.predict(self,input)\n",
    "        return ActivationTable(pred)\n",
    "    def have_next(self):\n",
    "        return ray.get(self.c.have_next.remote())\n",
    "    def predict(self,input):\n",
    "        input = self.preprocess(input)\n",
    "        pred = self.model.predict(input)\n",
    "        if not isinstance(pred, list):\n",
    "            pred = [pred]\n",
    "        for i in range(len(pred)):\n",
    "            pred[i] = pred[i][...,self.unit]\n",
    "        return pred\n",
    "    def preprocess(self, input):\n",
    "        seq = input\n",
    "        char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    "         '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    "         '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    "         'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    "         'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    "         'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "        't': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}\n",
    "        pad_char='~'\n",
    "        w_size = 30\n",
    "        step_size = 5\n",
    "        n_test_tuples = 128\n",
    "        if w_size == -1:\n",
    "            w_size = max(len(S) for S in sequences)\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        steps = range(step_size, len(seq), step_size)\n",
    "        if len(steps) == 0:\n",
    "            steps = [len(seq)-1]\n",
    "        for i in steps:\n",
    "            end = i\n",
    "            start = max(0, i-w_size)\n",
    "            s = seq[start:end]\n",
    "            assert len(s) <= w_size\n",
    "            x = s.rjust(w_size, pad_char)\n",
    "            assert len(x) == w_size\n",
    "            X.append(x)\n",
    "            y.append(seq[i])\n",
    "            if len(X) > n_test_tuples:\n",
    "                break\n",
    "        test_from = X\n",
    "        X_test, char2int = dni.tool.TwoDimEncoders.raw_to_bin_tensor(test_from, cust_char2int=char2int)\n",
    "        return X_test\n",
    "    \n",
    "#dynamicaaly build physical activation extractor based on logical activation extractor\n",
    "def build_physical_activation_ext(LogicalActivationExt):\n",
    "    @ray.remote\n",
    "    class PhysicalActivationExt(LogicalActivationExt):\n",
    "        def get_next(self):\n",
    "            if not self.have_next():\n",
    "                return None\n",
    "            input_table = ray.get(self.c.get_next.remote())\n",
    "            act_table = ActivationTable()\n",
    "            # read the whole batch of inputs\n",
    "            for num, input in input_table.itr():\n",
    "                act_table.add_table(self.predict(input))    \n",
    "            return act_table\n",
    "        def have_next(self):\n",
    "            return super().have_next()\n",
    "    return PhysicalActivationExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m 2019-11-17 21:43:52.446831: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m 2019-11-17 21:43:52.453956: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m 2019-11-17 21:43:52.454222: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c329aae7a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m 2019-11-17 21:43:52.454259: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m 2019-11-17 21:43:52.454527: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=24010)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation_shape': (30,), 'input_num': 79, 'layer_num': 1, 'unit_num': 4}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phy = build_physical_scanner(LocalScanner)\n",
    "ps = phy.remote('sqldata.txt',2)\n",
    "phyact = build_physical_activation_ext(ActivationExt)\n",
    "m = phyact.remote(ps,\"track_history/models-03-2.78.hdf5\",layers[0],units[0])\n",
    "actt = ray.get(m.get_next.remote())\n",
    "actt.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_shape': (30,), 'input_num': 79, 'layer_num': 1, 'unit_num': 4}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = LocalScanner.remote('sqldata.txt')\n",
    "ray.get(c.have_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=21646)\u001b[0m python3: ../nptl/pthread_mutex_lock.c:352: __pthread_mutex_lock_full: Assertion `INTERNAL_SYSCALL_ERRNO (e, __err) != ESRCH || !robust' failed.\n"
     ]
    }
   ],
   "source": [
    "c = LocalScanner.remote('sqldata.txt')\n",
    "it = InputTable([ray.get(c.get_next.remote())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_num': 1}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_num': 4}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.add_table([ray.get(c.get_next.remote()),ray.get(c.get_next.remote())])\n",
    "it.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  SELECT *, ix.* FROM )  SELECT pqohm.nl65p  AS bpxhg FROM yc11 WHERE p65.bn7<oo  AND ..false..<>.fj. ORDER BY ff DESC ) AS r08z union  SELECT .1875. * t8a5gu, false, wtl.* FROM vz AS rk4j6.* q36 AS w2orpr INNER JOIN l5  ON q3<=to4uao.tc WHERE f9c7kr<=azf8x GROUP BY  in7kmt$\n",
      "1  SELECT 38.22300, opbu.*, tlf.z03n2w, i6fc.*, b0.ssey  AS ydfygj FROM d4 AS nufxa WHERE cn6f.ssi4<=71.07001 LIMIT 8519 OFFSET 691$\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote\n",
    "class FeatureExtractor():\n",
    "    def __init__(self, accessmethod, intermediate_function, list_of_featurefunctions):\n",
    "        self.intermediate = intermediate_function\n",
    "        self.features = list_of_featurefunctions\n",
    "        self.c = accessmethod\n",
    "    def get_next(self):\n",
    "        if not self.have_next():\n",
    "            return None    \n",
    "        input = ray.get(self.c.get_next.remote())    \n",
    "        return self.extract(input)\n",
    "    def extract(self, input):\n",
    "        intermediate = self.intermediate(input)\n",
    "        features_task = []\n",
    "        # apply each feature function\n",
    "        for featurefunction in self.features:\n",
    "                features_task.append(ray.remote(featurefunction).remote(intermediate,input))\n",
    "        features = []\n",
    "        for i in range(len(features_task)):\n",
    "                features.append(ray.get(features_task[i]))\n",
    "        return features\n",
    "    def have_next(self):\n",
    "        return ray.get(self.c.have_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamicaaly build physical activation extractor based on logical activation extractor\n",
    "def build_physical_feature_ext(LogicalFeatureExt):\n",
    "    @ray.remote\n",
    "    class PhysicalFeatureExt(LogicalFeatureExt):\n",
    "        def get_next(self):\n",
    "            if not self.have_next():\n",
    "                return None\n",
    "            input_table = ray.get(self.c.get_next.remote())\n",
    "            f_table = dni.tool.FeatureTable()\n",
    "            # read the whole batch of inputs\n",
    "            for num, input in input_table.itr():\n",
    "                f_table.add_table(self.extract(input))    \n",
    "            return f_table\n",
    "        def have_next(self):\n",
    "            return super().have_next()\n",
    "    return PhysicalFeatureExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 2, 'feature_shape': (30,), 'input_num': 79}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phy = build_physical_scanner(LocalScanner)\n",
    "ps = phy.remote('sqldata.txt',2)\n",
    "phy_f_act = build_physical_feature_ext(FeatureExtractor)\n",
    "fxt = phy_f_act.remote(ps,generate_parsetree,[extractfeature1,extractfeature2])\n",
    "ftt = ray.get(fxt.get_next.remote())\n",
    "ftt.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 2, 'feature_shape': (30,), 'input_num': 79}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ll = [1,2,3,4]\n",
    "for i in ll:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FeatureExtractor(c1,generate_parsetree,[extractfeature1,extractfeature2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = generate_parsetree(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [extractfeature1(inter,sample_input),extractfeature2(inter,sample_input)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not initialized'"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 2, 'feature_shape': (30,), 'input_num': 80}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = [extractfeature1(inter,sample_input),extractfeature2(inter,sample_input)]\n",
    "ft.add_table(f)\n",
    "ft.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 30)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = phyact.remote(ps,\"track_history/models-03-2.78.hdf5\",layers[0],units[0])\n",
    "actt = ray.get(m.get_next.remote())\n",
    "actt.get_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 30)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = self.table\n",
    "        if table1 is None:\n",
    "            new_table = table2\n",
    "        else:\n",
    "            new_table = []\n",
    "            for i in range(layer_num):\n",
    "                activation_per_layer = []\n",
    "                for j in range(unit_num):\n",
    "                    activation_per_layer.append(np.concatenate((table1[i][...,j], table2[i][...,j])))\n",
    "                new_table.append(np.transpose(np.array(activation_per_layer), (1, 2, 0))) \n",
    "        self.__init__(new_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 30)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(layer_num):\n",
    "    self.table[i] = np.concatenate((self.table[0], newtable[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 1\n",
    "layer_num = 1\n",
    "unit_num = 4\n",
    "feature_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch mode exhaustive correlation\n",
    "class Incremental_Correlation():\n",
    "    def __init__(self, feature_ext, model_ext, model_num, layer_num, unit_num, feature_num):\n",
    "        import numpy as np\n",
    "        self.sumXsquare = np.zeros(shape=(model_num, layer_num, unit_num, feature_num))\n",
    "        self.sumYsquare = np.zeros(shape=(model_num, layer_num, unit_num, feature_num))\n",
    "        self.sumX = np.zeros(shape=(model_num, layer_num, unit_num, feature_num))\n",
    "        self.sumY = np.zeros(shape=(model_num, layer_num, unit_num, feature_num))\n",
    "        self.sumXY = np.zeros(shape=(model_num, layer_num, unit_num, feature_num))\n",
    "        self.n = np.full((model_num, layer_num, unit_num, feature_num),1e-17)\n",
    "        self.model_child = model_ext\n",
    "        self.feature_child = feature_ext\n",
    "    def open_itr(self):\n",
    "        while ray.get(self.model_child.have_next.remote()) is True and ray.get(self.feature_child.have_next.remote()) is True:\n",
    "            feature_table = ray.get(self.feature_child.get_next.remote())\n",
    "            feature_stat = feature_table.get_stat()\n",
    "            activation_table = ray.get(self.model_child.get_next.remote())\n",
    "            activation_stat = activation_table.get_stat()\n",
    "            if(feature_stat[\"input_num\"] != activation_stat[\"input_num\"]):\n",
    "                print(feature_stat[\"input_num\"], activation_stat[\"input_num\"])\n",
    "            assert(feature_stat[\"input_num\"] == activation_stat[\"input_num\"])\n",
    "            for i in range(feature_stat[\"input_num\"]):\n",
    "                for feature_num, feature in feature_table.itr(i):\n",
    "                    for layer_num, unit_num, activation in activation_table.itr(i):\n",
    "                        self.increment(feature,activation,0,layer_num,unit_num,feature_num)      \n",
    "    # input should from one layer one unit\n",
    "    def increment(self,input1,input2,model, layer, unit, feature):\n",
    "        import numpy as np\n",
    "        while input1.ndim != 1:\n",
    "            input1 = np.concatenate(input1)\n",
    "        while input2.ndim != 1:\n",
    "            input2 = np.concatenate(input2)\n",
    "        for i in range(len(input1)):\n",
    "            self.sumXsquare[model, layer, unit, feature] += input1[i]**2\n",
    "            self.sumYsquare[model, layer, unit, feature] += input2[i]**2\n",
    "            self.sumX[model, layer, unit, feature] += input1[i]\n",
    "            self.sumY[model, layer, unit, feature] += input2[i]\n",
    "            self.sumXY[model, layer, unit, feature] += input1[i]*input2[i]\n",
    "            self.n += 1\n",
    "    def extract(self,model, layer, unit, feature):\n",
    "        return (self.sumXY[model, layer, unit, feature] - self.sumX[model, layer, unit, feature]*self.sumY[model, layer, unit, feature]/\\\n",
    "                self.n[model, layer, unit, feature])/(((self.sumXsquare[model, layer, unit, feature] - self.sumX[model, layer, unit, feature]\\\n",
    "                **2/self.n[model, layer, unit, feature])**(1/2))*((self.sumYsquare[model, layer, unit, feature]\\\n",
    "                - self.sumY[model, layer, unit, feature]**2/self.n[model, layer, unit, feature])**(1/2))+1e-17)\n",
    "    def get_class(self):\n",
    "        return \"Batch_Incremental_Comprehensive_Correlation\"\n",
    "    def get_name(self):\n",
    "        return \"Correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:30:01,816\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.128.0.51',\n",
       " 'object_store_address': '/tmp/ray/session_2019-11-16_22-47-25_815266_4941/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-11-16_22-47-25_815266_4941/sockets/raylet',\n",
       " 'redis_address': '10.128.0.51:6379',\n",
       " 'session_dir': '/tmp/ray/session_2019-11-16_22-47-25_815266_4941',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO support layers, units\n",
    "import ray\n",
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "import numpy as np\n",
    "ray.shutdown()\n",
    "ray.init(address=\"10.128.0.51:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy = dni.access.build_physical_scanner(dni.access.LocalScanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:30:04,180\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(ec000fb708000000), class name = PhysicalScanner.\n",
      "2019-11-17 22:30:04,200\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(035640c908000000), class name = PhysicalScanner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m 2019-11-17 22:30:06.305075: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m 2019-11-17 22:30:06.311747: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m 2019-11-17 22:30:06.312087: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560f832d6440 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m 2019-11-17 22:30:06.312117: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m 2019-11-17 22:30:06.312371: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=26024)\u001b[0m \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |        Score         |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  | 0.03992677222423158  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  | 0.10710977167558725  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  | 0.053571646020853896 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  | 0.10934223866650748  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  | -0.03795637676873332 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.04900551556588565 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  | 0.04485307167406726  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  | 0.13366009792337266  |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m 2019-11-17 21:55:04.786133: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m 2019-11-17 21:55:04.793072: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m 2019-11-17 21:55:04.793447: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55759931f310 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m 2019-11-17 21:55:04.793478: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m 2019-11-17 21:55:04.793749: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=25286)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "phy = dni.build_physical_scanner(LocalScanner)\n",
    "ps1 = phy.remote('sqldata.txt',2)\n",
    "phy_f_act = build_physical_feature_ext(FeatureExtractor)\n",
    "fxt = phy_f_act.remote(ps1,generate_parsetree,[extractfeature1,extractfeature2])\n",
    "# ftt = ray.get(fxt.get_next.remote())\n",
    "# print(ftt.get_stat()[\"input_num\"])\n",
    "ps2 = phy.remote('sqldata.txt',2)\n",
    "phy_m_act = build_physical_activation_ext(ActivationExt)\n",
    "m = phy_m_act.remote(ps2,\"track_history/models-03-2.78.hdf5\",layers[0],units[0])\n",
    "# actt = ray.get(m.get_next.remote())\n",
    "# actt.get_stat()[\"input_num\"]\n",
    "corr_metric = IncrementalCorrelation(fxt,m,model_num,layer_num,unit_num,feature_num)\n",
    "corr_metric.open_itr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |        Score         |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  | 0.014518562762868671 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  |  0.1291405753793342  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  | 0.03177906175778336  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.1397928095421626  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  | -0.04019260657924972 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.08666947121118071 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  | 0.05281378493719408  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  |  0.1497857183992217  |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-23f06df25ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_itr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-f46d2dab39f3>\u001b[0m in \u001b[0;36mopen_itr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mactivation_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_child\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mactivation_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_num\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactivation_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(ray.remote(x).remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 03:39:31,539\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(4b48460002000000), class name = LocalScanner.\n",
      "2019-11-17 03:39:31,555\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(cd1f5c7d02000000), class name = FeatureExtractor.\n",
      "2019-11-17 03:39:31,557\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(20f84bc802000000), class name = LocalScanner.\n",
      "2019-11-17 03:39:31,569\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(dbc0c02402000000), class name = ActivationExt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m Using TensorFlow backend.\n",
      "{'feature_shape': (30,), 'feature_num': 2, 'input_num': 54}\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m 2019-11-17 03:39:34.372146: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m 2019-11-17 03:39:34.379115: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m 2019-11-17 03:39:34.379550: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562e68bd6b80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m 2019-11-17 03:39:34.379589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m 2019-11-17 03:39:34.379816: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=9183)\u001b[0m \n",
      "{'unit_num': 4, 'layer_num': 1, 'activation_shape': (30,), 'input_num': 54}\n"
     ]
    }
   ],
   "source": [
    "c1 = LocalScanner.remote('sqldata.txt')\n",
    "f = FeatureExtractor.remote(c1,generate_parsetree,[extractfeature1,extractfeature2])\n",
    "c2 = LocalScanner.remote('sqldata.txt')\n",
    "m = ActivationExt.remote(c2,\"track_history/models-03-2.78.hdf5\",layers[0],units[0])\n",
    "fff =ray.get(f.get_next.remote())\n",
    "print(fff.get_stat())\n",
    "mmm =ray.get(m.get_next.remote())\n",
    "print(mmm.get_stat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk=ray.get(f.get_next.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkk.get_stat()[\"input_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fda5fc37108>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) 1\n",
      "(0, 1) 2\n",
      "(1, 0) 3\n",
      "(1, 1) 4\n"
     ]
    }
   ],
   "source": [
    "for index, x in np.ndenumerate(a):\n",
    "    print(index, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "1 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for feature_num, feature in fff.get_features(0):\n",
    "    print(feature_num, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ActivationTable at 0x7fda603b4588>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_num': 2, 'feature_shape': (30,), 'input_num': 54}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_shape': (30,), 'input_num': 54, 'layer_num': 1, 'unit_num': 4}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc():\n",
    "    def __init__(self,i):\n",
    "        self.count = i\n",
    "    \n",
    "    def get_next(self):\n",
    "        self.count += 1\n",
    "        return self.count\n",
    "    def have_next(self):\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  scc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalScanner(AccessMethod):\n",
    "    def __init__(self, file):\n",
    "        self.file = file   \n",
    "    def get_next(self):\n",
    "        if self.have_next():\n",
    "            return self.file.read_next()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def have_next(self):\n",
    "        return self.file.have_next()\n",
    "            \n",
    "class PhysicalScanner(LogicalScanner):\n",
    "    def __init__(self,file,batch):\n",
    "        super().__init__(data)\n",
    "        self.batch = batch\n",
    "    \n",
    "    def get_next(self):\n",
    "        batch_inputs = []\n",
    "        while self.have_next() and len(batch_inputs)<self.batch:\n",
    "            batch_inputs.append(super().get_next())\n",
    "        return batch_inputs\n",
    "    \n",
    "    def have_next(self):\n",
    "        return super().have_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = physical_scanner(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parsetree(inputdata):\n",
    "    import warnings\n",
    "    import copy\n",
    "    import collections\n",
    "    import time, timeit\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import Tree\n",
    "    from nltk import PCFG,CFG\n",
    "    from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "    from nltk.probability import DictionaryProbDist\n",
    "    expr = inputdata\n",
    "    gram = None\n",
    "    with open('sql_full_XL.pcfg', 'r') as f:\n",
    "        pcfg_string = f.read()\n",
    "        from nltk import PCFG\n",
    "        gram = PCFG.fromstring(pcfg_string)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    ix = rule_index\n",
    "    len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "    counter = 0\n",
    "    term_index = {}\n",
    "    new_rules = []\n",
    "    for rule in gram.productions():\n",
    "        # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "        if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "            new_rules.append(rule)\n",
    "            continue\n",
    "        # Otherwise creates a new rule\n",
    "        new_rhs = []\n",
    "        for r in rule.rhs():\n",
    "            if is_nonterminal(r) :\n",
    "                new_rhs.append(r)\n",
    "            else:\n",
    "                if r not in term_index:\n",
    "                    new_left = Nonterminal('symb_'+str(r))\n",
    "                    prule = ProbabilisticProduction(new_left,\n",
    "                                                    [r],\n",
    "                                                    prob=1.0)\n",
    "                    term_index[r]=prule\n",
    "                new_rhs.append(new_left)\n",
    "        new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                           new_rhs,\n",
    "                                           prob=rule.prob())\n",
    "        new_rules.append(new_rule)\n",
    "    new_rules += term_index.values()\n",
    "    gram = PCFG(gram.start(), new_rules)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    old_grammar = gram\n",
    "    # BFS\n",
    "    arules = set()\n",
    "    visited = set()\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            arules.add(r)\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.add(symb)\n",
    "    # Creates new rules\n",
    "    arules = list(arules)\n",
    "    gram = PCFG(old_grammar.start(), arules)\n",
    "\n",
    "    symbols = set()\n",
    "    for rule in gram.productions():\n",
    "        symbols.add(rule.lhs())\n",
    "        for r in rule.rhs():\n",
    "            symbols.add(r)\n",
    "\n",
    "    \n",
    "    term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "    terminals = set(term_nodes)\n",
    "    L = max(len(w) for w in terminals)\n",
    "    def terminal(i, expr):\n",
    "        for wid in reversed(range(1, L+1)):\n",
    "            j = i + wid\n",
    "            if j > len(expr):\n",
    "                continue\n",
    "            w = expr[i:j]\n",
    "            if w in terminals:\n",
    "                return w\n",
    "        return None\n",
    "\n",
    "    tokens = []\n",
    "    index = []\n",
    "    i = 0\n",
    "    skipchars=['~']\n",
    "    while i < len(expr):\n",
    "        if not expr[i] in skipchars:\n",
    "            term = terminal(i, expr)\n",
    "            if term is None:\n",
    "                if not ignore_errors:\n",
    "                    raise ValueError('Could not match token', expr[i:])\n",
    "                else:\n",
    "                    print(\"null\")\n",
    "            tokens.append(term)\n",
    "            index.append((i,len(term)))\n",
    "            i += len(term)\n",
    "        else:\n",
    "            i += 1\n",
    "    parser = nltk.EarleyChartParser(gram)\n",
    "    parse = None\n",
    "    for p in parser.parse(tokens):\n",
    "        parse = p\n",
    "        break\n",
    "\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    # Gets the symbols BFS\n",
    "    visited = [gram.start()]\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.append(symb)\n",
    "\n",
    "    visited = list(reversed(visited))\n",
    "\n",
    "    # Alternative method for checking correctness\n",
    "    alt_visited = [S for S in symbols if is_nonterminal(S)]\n",
    "    assert set(visited) == set(alt_visited)\n",
    "\n",
    "    non_term_nodes = visited\n",
    "    tree = parse\n",
    "    n_tokens = len(tree.leaves())\n",
    "    nt_symbols = non_term_nodes\n",
    "\n",
    "    rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "    rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "    def visit(tree, offset):\n",
    "        if isinstance(tree, Tree):\n",
    "            n_tokens = 0\n",
    "            for subtree in tree:\n",
    "                n_tokens += visit(subtree, offset + n_tokens)\n",
    "            symb = tree.label()\n",
    "            j = rule2feat[symb]\n",
    "            rule_feats[offset:offset+n_tokens, j] = 1\n",
    "            return n_tokens\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    visit(tree, 0)\n",
    "    seq = expr\n",
    "    lex_index = index\n",
    "    w_feats = rule_feats\n",
    "\n",
    "\n",
    "    ch_len = len(seq)\n",
    "    w_len  = w_feats.shape[0]\n",
    "    n_feats = w_feats.shape[1]\n",
    "    ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "    for i_w in range(w_len):\n",
    "        f_w  = w_feats[i_w,...]\n",
    "        i_ch,len_ch = lex_index[i_w]\n",
    "        f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "        ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "    return [ch_feats,nt_symbols]\n",
    "\n",
    "# feature for \"where\" symbol in sql\n",
    "def extractfeature1(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[119])\n",
    "\n",
    "# feature for \"From\" symbol in sql\n",
    "def extractfeature2(intermiediate, inputdata):  \n",
    "    import numpy as np\n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress[96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "units = []\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "Models = [\"Models/boolean.h5\"]\n",
    "# FeaturesFunctions = features \n",
    "# FeatureNames = feature_names\n",
    "# FeatureExtractor = [F_Extractor]*6\n",
    "# MetricExtractor = [corr_metric] \n",
    "# MetricName = Metric_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 20:04:38,109\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-11-11 20:04:38,142\tWARNING actor.py:583 -- Actor is garbage collected in the wrong driver. Actor id = ActorID(5d29414501000000), class name = Scanner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.698390: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707196: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ce7b88010 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m 2019-11-11 20:04:42.707918: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=3356)\u001b[0m \n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |        Score         |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  | 0.11182267359630765  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  | 0.12861303059131324  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  | 0.14978813616253056  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.1254446026514693  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  | -0.09478801131318572 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.07576803250636789 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  |  0.1543156186363407  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  | 0.12079677432167589  |\n",
      "+-----------------------------------+-------------+---------+--------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "units = []\n",
    "ray.shutdown()\n",
    "ray.init(address=clusterid)\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])\n",
    "    \n",
    "maxlayer = 0\n",
    "maxunits = 0\n",
    "for i in range(len(Models)):\n",
    "    maxlayer = max(maxlayer,len(layers[i]))\n",
    "    maxunits = max(maxunits,len(units[i]))\n",
    "\n",
    "numfeature = 0\n",
    "for i in range(len(FeaturesFunctions)):\n",
    "    if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "        numfeature += len(FeaturesFunctions[i][0])\n",
    "    else:\n",
    "        numfeature += 1\n",
    "\n",
    "metrics = [None]*len(MetricName)\n",
    "for i in range(len(MetricName)):\n",
    "    if MetricName[i] == \"Correlation\":\n",
    "        metrics[i] = dni.metric.Batch_Incremental_Comprehensive_Correlation(len(Models),maxlayer,maxunits,numfeature)\n",
    "\n",
    "ModelActor = ActivationExt.remote(Models[0],layers[0])\n",
    "\n",
    "FeatActor = [None]*len(FeaturesFunctions)\n",
    "for i in range(len(FeaturesFunctions)):\n",
    "    # if has intermediate result\n",
    "    if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "        featureclass = ray.remote(dni.feature.CachedFeatureExtractor)\n",
    "        FeatActor[i] = featureclass.remote(FeaturesFunctions[i][0],FeaturesFunctions[i][1])\n",
    "    else:\n",
    "        FeatActor[i] = FeatureExtractor[i].remote(FeaturesFunctions[i],FeatureNames[i])\n",
    "\n",
    "ScannerActor = AccessMethod.remote()\n",
    "act = []\n",
    "feature = [[] for i in range(len(FeatureNames))]\n",
    "while ray.get(ScannerActor.HasMore.remote()):\n",
    "    data = ScannerActor.Next.remote()\n",
    "    Ext = ModelActor.extract.remote(data,units[0])\n",
    "    act.append(ray.get(Ext))\n",
    "    currf = 0\n",
    "    # for each group of feature functions\n",
    "    for i in range(len(FeaturesFunctions)):\n",
    "        if type(FeaturesFunctions[i]) is list and len(FeaturesFunctions[i]) == 2:\n",
    "            FeatActor[i].setup.remote(data)\n",
    "            # for each feature functions\n",
    "            for j in range(len(FeaturesFunctions[i][0])):\n",
    "                FeatActor[i].extract.remote(data,1)\n",
    "                feature = ray.get(FeatActor[i].extract.remote(data,j))\n",
    "                # for each layer\n",
    "                for k in range(len(layers[0])):\n",
    "                    # for each units\n",
    "                    for l in range(len(units[i])):\n",
    "                        # for each metric\n",
    "                        for m in range(len(MetricName)):\n",
    "                            metrics[i].increment(feature,ray.get(Ext)[k][...,l],0,k,l,currf)\n",
    "                currf += 1\n",
    "        else:\n",
    "            feature[currf].append(ray.get(FeatActor[i].extract.remote(data)))\n",
    "            currf += 1\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "t = PrettyTable(['Model','Metric','Feature','Neuron','Score'])\n",
    "\n",
    "#for each model\n",
    "for i in range(len(Models)):\n",
    "    # for each layer\n",
    "    for j in range(len(layers[i])):\n",
    "        # for each unit\n",
    "        for k in range(len(units[i])):\n",
    "            # for each feature\n",
    "            for l in range(numfeature):\n",
    "                t.add_row([Models[i],MetricName[j],FeatureNames[l],\"(\"+str(layers[i][j])+\",\"+str(units[i][k])+\")\",metrics[i].extract(0,j,k,l)])\n",
    "                \n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:37.846780: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353038: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353405: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c7c4131790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.353437: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m 2019-11-11 17:48:38.354687: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=2004)\u001b[0m \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Actor(CachedFeatureExtractor, c53598d002000000)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = []\n",
    "#for each model\n",
    "for i in range(len(Models)):\n",
    "    metric_temp1 = []\n",
    "    # for each layer\n",
    "    for j in range(len(layers[i])):\n",
    "        # for each unit\n",
    "        for k in range(len(units[i])):\n",
    "            # for each feature\n",
    "            for l in range(len(FeatureNames)):\n",
    "                \n",
    "                \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30, 4)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each load of input data, each layer, get a number of activation, the last dimension is unit\n",
    "act[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each feature, each load of input data, get a number of features\n",
    "feature[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "|               Model               |    Metric   | Feature | Neuron |         Score         |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,0)  |  0.02771628520146099  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,0)  |  0.11961696236126476  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,1)  |  0.05012749261812874  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,1)  |  0.12826532216877126  |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,2)  |  -0.04503853262076593 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,2)  | -0.038613080219511146 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |  WHERE  | (0,3)  |  0.059537094383411385 |\n",
      "| track_history/models-03-2.78.hdf5 | Correlation |   FROM  | (0,3)  |  0.13883371936802053  |\n",
      "+-----------------------------------+-------------+---------+--------+-----------------------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scanner.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(s.HasMore.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SELECT *, ix.* FROM )  SELECT pqohm.nl65p  AS bpxhg FROM yc11 WHERE p65.bn7<oo  AND ..false..<>.fj. ORDER BY ff DESC ) AS r08z union  SELECT .1875. * t8a5gu, false, wtl.* FROM vz AS rk4j6.* q36 AS w2orpr INNER JOIN l5  ON q3<=to4uao.tc WHERE f9c7kr<=azf8x GROUP BY  in7kmt$'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines  = []\n",
    "while ray.get(s.HasMore.remote()):\n",
    "    lines.append(s.Next.remote())\n",
    "ray.get(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = ray.get(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 171)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate = generate_parsetree(ray.get(lines[1]))\n",
    "intermediate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in enumerate(intermediate[1]):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 30)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = extractfeature1(intermediate,ray.get(lines[1]))\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureext = ray.remote(dni.feature.CachedFeatureExtractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureext = featureext.remote([extractfeature1,extractfeature2],generate_parsetree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(ea7106654faf4b49c2340100000000c001000000)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureext.setup.remote(ray.get(lines[0]))\n",
    "ray.get(featureext.extract.remote(lines[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-260-54a76c4220d1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-260-54a76c4220d1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a = 1,b = 2\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = [[[0],[0,1,2,3]]]\n",
    "Models = [\"track_history/models-03-2.78.hdf5\"]\n",
    "clusterid = \"10.128.0.51:6379\"\n",
    "AccessMethod= Scanner\n",
    "ActivationExt = ActivationExt\n",
    "Models = [\"Models/boolean.h5\"]\n",
    "FeaturesFunctions = features \n",
    "FeatureNames = feature_names\n",
    "FeatureExtractor = [F_Extractor]*6\n",
    "MetricExtractor = [corr_metric] MetricName = Metric_names\n",
    "layers = []\n",
    "units = []\n",
    "for i in range(len(Neuron)):\n",
    "    layers.append(Neuron[i][0])\n",
    "    units.append(Neuron[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-10 20:16:22,983\tWARNING worker.py:1619 -- The actor or task with ID ffffffffffffd1cda02401000000 is pending and cannot currently be scheduled. It requires {} for execution and {CPU: 1.000000} for placement, but this node only has remaining {object_store_memory: 2.832031 GiB}, {CPU: 4.000000}, {memory: 8.251953 GiB}. In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.174985: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.182514: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.182985: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b48256a730 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.183017: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m 2019-11-10 20:16:25.183389: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=18136)\u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 30, 4)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelActor = ActivationExt.remote(Models[0],layers[0])\n",
    "act1 = ray.get(ModelActor.extract.remote(lines[1],units[0]))\n",
    "act1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
