{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 23:56:17,504\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.128.0.51',\n",
       " 'object_store_address': '/tmp/ray/session_2019-11-05_16-45-18_491016_1575/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-11-05_16-45-18_491016_1575/sockets/raylet',\n",
       " 'redis_address': '10.128.0.51:6379',\n",
       " 'session_dir': '/tmp/ray/session_2019-11-05_16-45-18_491016_1575',\n",
       " 'webui_url': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO support layers, units\n",
    "import ray\n",
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "import numpy as np\n",
    "ray.shutdown()\n",
    "ray.init(address=\"10.128.0.51:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Scanner(dni.access.Scan):\n",
    "    def __init__(self):\n",
    "        from google.cloud import storage\n",
    "        import numpy as np\n",
    "        storage_client = storage.Client()\n",
    "        bucketn = \"dni-storage\"\n",
    "        bucket = storage_client.get_bucket(bucketn)\n",
    "        self.blob = bucket.get_blob('Datasets/data.txt')\n",
    "        self.curr = 0\n",
    "        self.step = 128 # fetch how many data at one time\n",
    "    def Next(self):\n",
    "        if not self.HasMore():\n",
    "            return \"\"\n",
    "        # fetch data\n",
    "        s = self.blob.download_as_string(start=self.curr*51*self.step, \n",
    "                                         end=self.curr*51*self.step+51*self.step-1)\n",
    "        self.curr += 1\n",
    "        return self.process_raw(s)\n",
    "\n",
    "    def HasMore(self):\n",
    "        return self.curr * self.step < 4992\n",
    "    \n",
    "    def process_raw(self,s):\n",
    "        s = s.decode(\"utf-8\") \n",
    "        s = s.split(\"\\n\")\n",
    "        if s[-1] == '':\n",
    "            del s[-1]\n",
    "        char2int = {'X': 0, '0':1, '1':2, '|':3, '&':4}\n",
    "        encoded_seqs = [[char2int[x] for x in seq] for seq in s]\n",
    "        n_chars = max(char2int.values()) + 1\n",
    "        seq_len = len(encoded_seqs[0])\n",
    "        X = np.zeros((len(encoded_seqs), seq_len, n_chars), dtype=np.int)\n",
    "        for i, enc_seq in enumerate(encoded_seqs):\n",
    "            for j, x in enumerate(enc_seq):\n",
    "                X[i, j, x] = 1\n",
    "        return X\n",
    "    \n",
    "@ray.remote\n",
    "class Act_extract(dni.model.Extractor):\n",
    "    def __init__(self, modelname, layer):\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(\"dni-storage\")\n",
    "        blob = bucket.get_blob(modelname)\n",
    "        import os.path\n",
    "        if not os.path.isfile(modelname.split(\"/\")[-1]):\n",
    "            blob.download_to_filename(modelname.split(\"/\")[-1])\n",
    "        from keras.models import Model, load_model\n",
    "        self.model = load_model(modelname.split(\"/\")[-1])\n",
    "        self.model._make_predict_function()\n",
    "        outputs = [self.model.layers[l].output for l in layer]\n",
    "        self.model = Model(inputs = self.model.input, outputs = outputs)\n",
    "    def extract(self,input,unit):\n",
    "        pred = self.model.predict(input)\n",
    "        if not isinstance(pred, list):\n",
    "            pred = [pred]\n",
    "        for i in range(len(pred)):\n",
    "            pred[i] = pred[i][...,unit]\n",
    "        return pred\n",
    "    \n",
    "# feature builder    \n",
    "def fsm_states(transition_table, dictionary, init_state = 0, id_fsm = None):\n",
    "    def F(seq_raw):\n",
    "        int2char = {0:'X', 1:'0', 2:'1', 3:'|', 4:'&'}\n",
    "        seq = ''\n",
    "        #latter not 0\n",
    "        for i in seq_raw:\n",
    "            for k in range(len(i)):\n",
    "                if i[k] == 1:\n",
    "                    seq = seq + int2char[k]\n",
    "        features = np.zeros(len(seq))\n",
    "        cur_state = init_state\n",
    "        for i, x in enumerate(seq):\n",
    "            cur_state = transition_table[cur_state][dictionary[x]]\n",
    "            features[i] = cur_state\n",
    "        return features\n",
    "    return F\n",
    "\n",
    "fsm_dict = {'X': 0, '0':1, '1':2, '|':3, '&':4}\n",
    "fsm_tbl =  [[0, 2, 1, -1, -1],\n",
    "            [0, -1, -1, 4, 3],\n",
    "            [0, -1, -1, 6, 5],\n",
    "            [0, 2, 1, -1, -1],\n",
    "            [0, 1, 1, -1, -1],\n",
    "            [0, 2, 2, -1, -1],\n",
    "            [0, 2, 1, -1, -1]]\n",
    "fsm_states_7 = fsm_states(fsm_tbl, fsm_dict, id_fsm = '7')\n",
    "\n",
    "fsm_tbl4 = [[0,2,1,-1,-1],\n",
    "            [0,1,1,1,3],\n",
    "            [0,2,2,3,2],\n",
    "            [0,2,1,2,1]]\n",
    "fsm_states_4 = fsm_states(fsm_tbl4, fsm_dict, id_fsm = '4')\n",
    "\n",
    "\n",
    "fsm_tbl3 = [[0,2,1,-1,-1],\n",
    "           [0,2,1,1,0],\n",
    "           [0,2,2,0,2]]\n",
    "fsm_states_3 = fsm_states(fsm_tbl3, fsm_dict, id_fsm = '3')\n",
    "\n",
    "\n",
    "features = [fsm_states_7, fsm_states_4, fsm_states_3]\n",
    "\n",
    "def random_fsm(n_states, fsm_dict=fsm_dict):\n",
    "    s = np.random.randint(n_states, size=n_states*len(fsm_dict))\n",
    "    tran = np.reshape(s, (n_states, len(fsm_dict)))\n",
    "    return tran\n",
    "\n",
    "fsm_states_3rd = fsm_states(random_fsm(3), fsm_dict, id_fsm = 'rd_3')\n",
    "fsm_states_4rd = fsm_states(random_fsm(4), fsm_dict, id_fsm = 'rd_4')\n",
    "fsm_states_7rd = fsm_states(random_fsm(7), fsm_dict, id_fsm = 'rd_7')\n",
    "\n",
    "features += [fsm_states_3rd, fsm_states_4rd, fsm_states_7rd]\n",
    "feature_names = [\"states_7\", \"states_4\", \"states_3\",\"states_3rd\",\"states_4rd\",\"states_7rd\"]\n",
    "\n",
    "@ray.remote  \n",
    "class F_Extractor(dni.feature.Batch_Feature_Extractor):\n",
    "    pass\n",
    "    \n",
    "@ray.remote \n",
    "class corr_metric(dni.metric.Batch_Exhaustive_Correlation):\n",
    "    pass\n",
    "\n",
    "Metric_names = [\"Corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-05 23:56:29,493\tWARNING worker.py:1268 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m 2019-11-05 23:56:34.091886: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m 2019-11-05 23:56:34.099726: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m 2019-11-05 23:56:34.100091: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c6979d1b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m 2019-11-05 23:56:34.100117: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m 2019-11-05 23:56:34.100472: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=7973)\u001b[0m \n",
      "+-------------------+--------+------------+--------+-----------------------+\n",
      "|       Model       | Metric |  Feature   | Neuron |         Score         |\n",
      "+-------------------+--------+------------+--------+-----------------------+\n",
      "| Models/boolean.h5 |  Corr  |  states_7  | (0,0)  |  0.34056969772108825  |\n",
      "| Models/boolean.h5 |  Corr  |  states_4  | (0,0)  |   0.5108451076628742  |\n",
      "| Models/boolean.h5 |  Corr  |  states_3  | (0,0)  |   0.4080477708620373  |\n",
      "| Models/boolean.h5 |  Corr  | states_3rd | (0,0)  |   0.067454175778016   |\n",
      "| Models/boolean.h5 |  Corr  | states_4rd | (0,0)  |  -0.08189626464878833 |\n",
      "| Models/boolean.h5 |  Corr  | states_7rd | (0,0)  |  0.16999641977805796  |\n",
      "| Models/boolean.h5 |  Corr  |  states_7  | (0,1)  |  0.04836190930617687  |\n",
      "| Models/boolean.h5 |  Corr  |  states_4  | (0,1)  |  -0.05841510834432777 |\n",
      "| Models/boolean.h5 |  Corr  |  states_3  | (0,1)  |  -0.2041188792865357  |\n",
      "| Models/boolean.h5 |  Corr  | states_3rd | (0,1)  |  -0.36454532050211186 |\n",
      "| Models/boolean.h5 |  Corr  | states_4rd | (0,1)  |  0.15298408507625313  |\n",
      "| Models/boolean.h5 |  Corr  | states_7rd | (0,1)  |  0.17543782823858006  |\n",
      "| Models/boolean.h5 |  Corr  |  states_7  | (0,2)  | -0.058012313859639494 |\n",
      "| Models/boolean.h5 |  Corr  |  states_4  | (0,2)  |  -0.23732040735222787 |\n",
      "| Models/boolean.h5 |  Corr  |  states_3  | (0,2)  |  -0.29294439045381654 |\n",
      "| Models/boolean.h5 |  Corr  | states_3rd | (0,2)  |  -0.1538017654151749  |\n",
      "| Models/boolean.h5 |  Corr  | states_4rd | (0,2)  |  0.034438473149821916 |\n",
      "| Models/boolean.h5 |  Corr  | states_7rd | (0,2)  |   0.056857983306878   |\n",
      "| Models/boolean.h5 |  Corr  |  states_7  | (0,3)  |  -0.1265589977496405  |\n",
      "| Models/boolean.h5 |  Corr  |  states_4  | (0,3)  |  -0.2217454443542483  |\n",
      "| Models/boolean.h5 |  Corr  |  states_3  | (0,3)  |  -0.3752386289031937  |\n",
      "| Models/boolean.h5 |  Corr  | states_3rd | (0,3)  |  -0.20653230501154213 |\n",
      "| Models/boolean.h5 |  Corr  | states_4rd | (0,3)  |  0.08374207912262069  |\n",
      "| Models/boolean.h5 |  Corr  | states_7rd | (0,3)  |  0.029091720851343962 |\n",
      "+-------------------+--------+------------+--------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "import dni\n",
    "import importlib\n",
    "importlib.reload(dni)\n",
    "# system decides whether cache, batch\n",
    "# feature factory, batching, caching\n",
    "# do it for the sql\n",
    "# for feature factory, feature\n",
    "\n",
    "#\n",
    "dni.inspect(clusterid = \"10.128.0.51:6379\",\n",
    "            AccessMethod= Scanner,\n",
    "            ActivationExt = Act_extract, Neuron = [[[0],[0,1,2,3]]], \n",
    "            Models = [\"Models/boolean.h5\"],\n",
    "            FeaturesFunctions = features, FeatureNames = feature_names, \n",
    "            FeatureExtractor = [F_Extractor]*6,\n",
    "            MetricExtractor = [corr_metric], MetricName = Metric_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
