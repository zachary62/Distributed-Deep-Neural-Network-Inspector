{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"name\" : \"default\",\n",
    "\n",
    "    \"n_runs\"      : 1,\n",
    "    \"n_units\"     : [128],\n",
    "    \"cfg_files\"   : [\"sql_full_XL.pcfg\"],\n",
    "    \"n_tup_test\"  : [128],\n",
    "\n",
    "    \"n_epochs\"    : 50,\n",
    "    \"n_layers\"    : [1],\n",
    "    \"batch_size\"  : 128,\n",
    "    \"window_size\" : 30,\n",
    "    \"val_split\"   : 0.1,\n",
    "    \"patience\"    : 3,\n",
    "    \"max_seq_size\": 350,\n",
    "    \"n_seq_train\" : 128,\n",
    "    \"n_seq_test\"  : 100,\n",
    "    \"test_step\"   : 5,\n",
    "    \"T_max\"       : 1800,\n",
    "    \"DNI\" : [\n",
    "        {\n",
    "            \"version\": \"vanilla\",\n",
    "            \"metric\": \"log_regression\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import copy\n",
    "import collections\n",
    "import time, timeit\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "from nltk import PCFG,CFG\n",
    "from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "from nltk.probability import DictionaryProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_NAME_PREFIX='r_'\n",
    "\n",
    "def check_rules(ruleset):\n",
    "    pass\n",
    "class Grammar:\n",
    "\n",
    "    def __init__(self, pcfg_string=None, cfg_string=None,\n",
    "                       pcfg_file = None, cfg_file = None, sep_char=''):\n",
    "        # Reads files\n",
    "        if pcfg_file is not None:\n",
    "            with open(pcfg_file, 'r') as f:\n",
    "                print('Loading', pcfg_file)\n",
    "                pcfg_string = f.read()\n",
    "        if cfg_file is not None:\n",
    "            with open(cfg_file, 'r') as f:\n",
    "                print('Loading', cfg_file)\n",
    "                cfg_string = f.read()\n",
    "        self.sep_char = sep_char\n",
    "        self.pcfg_string = pcfg_string\n",
    "        self.cfg_string  = cfg_string\n",
    "        self.build_grammar()\n",
    "        self.wrap_terminals()\n",
    "        self.clean_grammar()\n",
    "        self.build_parser()\n",
    "        self.sort_non_term_nodes()\n",
    "\n",
    "    def wrap_terminals(self):\n",
    "        # Gets number of lhs per non terminal symbols\n",
    "        ix = self.rule_index()\n",
    "        len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "\n",
    "        counter = 0\n",
    "        term_index = {}\n",
    "        new_rules = []\n",
    "\n",
    "        for rule in self.grammar.productions():\n",
    "\n",
    "            # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "            if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "                new_rules.append(rule)\n",
    "                continue\n",
    "\n",
    "            # Otherwise creates a new rule\n",
    "            new_rhs = []\n",
    "            for r in rule.rhs():\n",
    "                if is_nonterminal(r) :\n",
    "                    new_rhs.append(r)\n",
    "                else:\n",
    "                    if r not in term_index:\n",
    "                        new_left = Nonterminal('symb_'+str(r))\n",
    "                        prule = ProbabilisticProduction(new_left,\n",
    "                                                        [r],\n",
    "                                                        prob=1.0)\n",
    "                        term_index[r]=prule\n",
    "                    new_rhs.append(new_left)\n",
    "\n",
    "            new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                               new_rhs,\n",
    "                                               prob=rule.prob())\n",
    "            new_rules.append(new_rule)\n",
    "\n",
    "        new_rules += term_index.values()\n",
    "\n",
    "        self.grammar = PCFG(self.grammar.start(), new_rules)\n",
    "\n",
    "    def rule_index(self):\n",
    "        rules = [r for r in self.grammar.productions() if r.prob() > 0]\n",
    "        rule_index = collections.defaultdict(list)\n",
    "        for r in rules:\n",
    "            rule_index[r.lhs()].append(r)\n",
    "        return rule_index\n",
    "\n",
    "    def sort_non_term_nodes(self):\n",
    "        # Creates rules index\n",
    "        rule_index = self.rule_index()\n",
    "\n",
    "        # Gets the symbols BFS\n",
    "        visited = [self.grammar.start()]\n",
    "        Q = collections.deque()\n",
    "        Q.append(self.grammar.start())\n",
    "\n",
    "        while len(Q) > 0:\n",
    "            # Gets first symbol in queue\n",
    "            symb = Q.popleft()\n",
    "            # Fetches corresponding rules\n",
    "            rules = rule_index[symb]\n",
    "            for r in rules:\n",
    "                for symb in r.rhs():\n",
    "                    if is_nonterminal(symb) and not symb in visited:\n",
    "                        Q.append(symb)\n",
    "                        visited.append(symb)\n",
    "\n",
    "        visited = list(reversed(visited))\n",
    "\n",
    "        # Alternative method for checking correctness\n",
    "        alt_visited = [S for S in self.symbols() if is_nonterminal(S)]\n",
    "        assert set(visited) == set(alt_visited)\n",
    "\n",
    "        self.sorted_non_term_nodes = visited\n",
    "\n",
    "    def build_grammar(self):\n",
    "        # Loads rules\n",
    "        if self.pcfg_string is not None:\n",
    "            self.grammar = PCFG.fromstring(self.pcfg_string)\n",
    "        elif self.cfg_string is not None:\n",
    "            self.load_cfg()\n",
    "        n_rules = len(self.grammar.productions())\n",
    "        print('Loaded grammar with', n_rules, 'rules')\n",
    "\n",
    "    def clean_grammar(self):\n",
    "        # Removes 0-proba rules\n",
    "        print('Grammar rules before removing zero-rules:', \\\n",
    "                len(self.grammar.productions()))\n",
    "        old_grammar = self.grammar\n",
    "\n",
    "        # Creates rules index\n",
    "        rule_index = self.rule_index()\n",
    "\n",
    "        # BFS\n",
    "        arules = set()\n",
    "        visited = set()\n",
    "        Q = collections.deque()\n",
    "        Q.append(self.grammar.start())\n",
    "\n",
    "        while len(Q) > 0:\n",
    "            # Gets first symbol in queue\n",
    "            symb = Q.popleft()\n",
    "            # Fetches corresponding rules\n",
    "            rules = rule_index[symb]\n",
    "            for r in rules:\n",
    "                arules.add(r)\n",
    "                for symb in r.rhs():\n",
    "                    if is_nonterminal(symb) and not symb in visited:\n",
    "                        Q.append(symb)\n",
    "                        visited.add(symb)\n",
    "\n",
    "        # Creates new rules\n",
    "        arules = list(arules)\n",
    "        check_rules(arules)\n",
    "        self.grammar = PCFG(old_grammar.start(), arules)\n",
    "        print('Grammar rules after removing zero-rules:', \\\n",
    "                len(self.grammar.productions()))\n",
    "\n",
    "    def n_rules(self):\n",
    "        return len(self.grammar.productions())\n",
    "\n",
    "    def n_features(self):\n",
    "        return len(self.non_term_nodes())*2\n",
    "\n",
    "    def build_parser(self):\n",
    "        # Creates parser\n",
    "        self.parser = nltk.EarleyChartParser(self.grammar)\n",
    "        print ('Created parser')\n",
    "\n",
    "    def load_cfg(self):\n",
    "        # Creates deterministic grammar\n",
    "        cfg_gram = CFG.fromstring(self.cfg_string)\n",
    "\n",
    "        # Groups by LHS\n",
    "        ggroups = collections.defaultdict(list)\n",
    "        for rule in cfg_gram.productions():\n",
    "            ggroups[rule.lhs()].append(rule)\n",
    "\n",
    "        # For each group computes probabilities\n",
    "        prules = []\n",
    "        for lhs, ruleset in ggroups.iteritems():\n",
    "            N = len(ruleset)\n",
    "            assert N > 0\n",
    "            for rule in ruleset:\n",
    "                p = 1.0 / N\n",
    "                prule = ProbabilisticProduction(rule.lhs(),\n",
    "                                                rule.rhs(),\n",
    "                                                prob=p)\n",
    "                prules.append(prule)\n",
    "\n",
    "        # Asserts that all probas add up to 1\n",
    "        check_rules(prules)\n",
    "\n",
    "        # Returns new grammar\n",
    "        self.grammar = PCFG(cfg_gram.start(), prules)\n",
    "\n",
    "    def symbols(self):\n",
    "        symbols = set()\n",
    "        for rule in self.grammar.productions():\n",
    "            symbols.add(rule.lhs())\n",
    "            for r in rule.rhs():\n",
    "                symbols.add(r)\n",
    "        return symbols\n",
    "\n",
    "    def term_nodes(self):\n",
    "        return [S for S in self.symbols() if is_terminal(S)]\n",
    "\n",
    "    def non_term_nodes(self):\n",
    "        return self.sorted_non_term_nodes\n",
    "\n",
    "    def dump_to_file(self, file):\n",
    "        '''\n",
    "            Trims grammar to max N rules\n",
    "            The RHS of the deleted tules will be transformed into terminals\n",
    "        '''\n",
    "        # In case already trimmed, reinstaures old grammar\n",
    "\n",
    "        # Walks the grammar BFS style until found N nodes\n",
    "        Q = collections.deque()\n",
    "        Q.append(self.grammar.start())\n",
    "        rules_visited = []\n",
    "        explored_rhs = set()\n",
    "\n",
    "        while len(Q) > 0:\n",
    "\n",
    "            # Gets first symbol in queue\n",
    "            symb = Q.popleft()\n",
    "            # Fetches corresponding rules\n",
    "            rules = self.grammar.productions(lhs=symb)\n",
    "\n",
    "            for rule in rules:\n",
    "\n",
    "                # Adds rule to collection\n",
    "                if not rule in rules_visited:\n",
    "                    rules_visited.append(rule)\n",
    "\n",
    "                # Adds right-hand symbols to queue\n",
    "                for rsymb in rule.rhs():\n",
    "                    if is_nonterminal(rsymb) and not rsymb in explored_rhs:\n",
    "                        Q.append(rsymb)\n",
    "                        explored_rhs.add(symb)\n",
    "\n",
    "        with open(file, 'w+') as f:\n",
    "            for rule in rules_visited:\n",
    "                f.write(str(rule) +'\\n')\n",
    "\n",
    "    def pruning_dag(self):\n",
    "        name_to_ix_1 = {n:2*i for i,n in enumerate(self.non_term_nodes())}\n",
    "        name_to_ix_2 = {n:2*i+1 for i,n in enumerate(self.non_term_nodes())}\n",
    "        to_feat_1 = lambda n: name_to_ix_1[n]\n",
    "        to_feat_2 = lambda n: name_to_ix_2[n]\n",
    "        return pruning.FeatureDAG(self.grammar,\n",
    "            [to_feat_1, to_feat_2], is_nonterminal)\n",
    "\n",
    "        # # Materializes the predcedence graph between rules\n",
    "        # tree = collections.defaultdict(set)\n",
    "        # for r in self.grammar.productions():\n",
    "        #     lf = to_node(r.lhs())\n",
    "        #     for rhs in r.rhs():\n",
    "        #         if is_nonterminal(rhs):\n",
    "        #             rf = to_node(rhs)\n",
    "        #             tree[lf].add(rf)\n",
    "\n",
    "        # first_node = to_node(self.grammar.start)\n",
    "\n",
    "        # return tree, first_node\n",
    "\n",
    "    ############\n",
    "    # Trimming #\n",
    "    ############\n",
    "    def freeze_symbol(self, node):\n",
    "        symb = node.symbol()\n",
    "        if self.sep_char is not None and self.sep_char != '':\n",
    "            return [symb]\n",
    "        else:\n",
    "            return [c for c in symb] + [' ']\n",
    "\n",
    "    def trim(self, N):\n",
    "        '''\n",
    "            Trims grammar to max N rules\n",
    "            The RHS of the deleted tules will be transformed into terminals\n",
    "        '''\n",
    "        # In case already trimmed, reinstaures old grammar\n",
    "        self.build_grammar()\n",
    "\n",
    "        # Walks the grammar BFS style until found N nodes\n",
    "        Q = collections.deque()\n",
    "        Q.append(self.grammar.start())\n",
    "        n = 0\n",
    "        rules_to_keep = []\n",
    "        explored_rhs = set()\n",
    "\n",
    "        while n < N and len(Q) > 0:\n",
    "            # Gets first symbol in queue\n",
    "            symb = Q.popleft()\n",
    "            # Fetches corresponding rules\n",
    "            rules = self.grammar.productions(lhs=symb)\n",
    "            for rule in rules:\n",
    "                # Makes sure that there's space left\n",
    "                if n >= N:\n",
    "                    break\n",
    "                # Adds rule to collection\n",
    "                if not rule in rules_to_keep:\n",
    "                    rules_to_keep.append(rule)\n",
    "                    n += 1\n",
    "                # Adds right-hand symbols to queue\n",
    "                for rsymb in rule.rhs():\n",
    "                    if is_nonterminal(rsymb) and not rsymb in explored_rhs:\n",
    "                        Q.append(rsymb)\n",
    "                # Marks symb as explored\n",
    "                explored_rhs.add(symb)\n",
    "\n",
    "        # Casts all the unresolved rules to terminals\n",
    "        for i,rule in enumerate(rules_to_keep):\n",
    "            new_rhs = []\n",
    "            for symb in rule.rhs():\n",
    "                if is_nonterminal(symb) and not symb in explored_rhs:\n",
    "                    symb = self.freeze_symbol(symb)\n",
    "                else:\n",
    "                    symb = [symb]\n",
    "                new_rhs.extend(symb)\n",
    "            new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                               new_rhs,\n",
    "                                               prob=rule.prob())\n",
    "            rules_to_keep[i] = new_rule\n",
    "\n",
    "        # Groups all rules that have the lhs\n",
    "        # This will be useful for everything that follows\n",
    "        rules_index = collections.defaultdict(list)\n",
    "        for i,rule in enumerate(rules_to_keep):\n",
    "            rules_index[rule.lhs()].append((i,rule))\n",
    "\n",
    "        # Removes infinite loops\n",
    "        for lhs, rules in rules_index.iteritems():\n",
    "\n",
    "            # Checks if the rules contains ONLY recursive rules\n",
    "            only_recurs = True\n",
    "            i_last = None\n",
    "            for ix,rule in rules:\n",
    "                is_recurs = any(s==lhs for s in rule.rhs())\n",
    "                only_recurs &= is_recurs\n",
    "                if is_recurs: i_last = ix\n",
    "\n",
    "            # If so, removes the recursivity in the last rule\n",
    "            if only_recurs:\n",
    "                last_r = rules_to_keep[i_last]\n",
    "                new_rhs = []\n",
    "                for symb in last_r.rhs():\n",
    "                    symb = self.freeze_symbol(symb) if symb==lhs else [symb]\n",
    "                    new_rhs.extend(symb)\n",
    "                rules_to_keep[i_last] = ProbabilisticProduction(\n",
    "                        last_r.lhs(), new_rhs, prob=last_r.prob())\n",
    "\n",
    "        # Rebuilds index\n",
    "        rules_index = collections.defaultdict(list)\n",
    "        for i,rule in enumerate(rules_to_keep):\n",
    "            rules_index[rule.lhs()].append((i,rule))\n",
    "\n",
    "        # Corrects the probabilites so that they all sum up to 1\n",
    "        for lhs, rules in rules_index.iteritems():\n",
    "            for r in rules:\n",
    "                tot_prob = sum(r.prob() for i,r in rules)\n",
    "                if abs(tot_prob - 1.0) > PCFG.EPSILON:\n",
    "                    for i,r in rules:\n",
    "                        rules_to_keep[i] = ProbabilisticProduction(\n",
    "                            r.lhs(), r.rhs(), prob=r.prob() / tot_prob)\n",
    "\n",
    "        # Replaces the grammar\n",
    "        check_rules(rules_to_keep)\n",
    "        self.grammar = PCFG(self.grammar.start(), rules_to_keep)\n",
    "        print('Trimmed grammar to', len(self.grammar.productions()), 'rules')\n",
    "        self.build_parser()\n",
    "\n",
    "    ####################################\n",
    "    # Sequence generation and sampling #\n",
    "    ####################################\n",
    "    def sample_tree(self):\n",
    "        global N_RECURS\n",
    "        N_RECURS = 0\n",
    "\n",
    "        def sample_from_lhs(symb):\n",
    "            # Checks recursivity\n",
    "            global N_RECURS\n",
    "            N_RECURS += 1\n",
    "            if N_RECURS > 5000:\n",
    "                raise Exception('Too many recursions!')\n",
    "\n",
    "            # Gets the prod rules with symb the left side\n",
    "            all_rules = rule_index[symb]\n",
    "\n",
    "            # Samples a rule\n",
    "            distrib = {r:r.prob() for r in all_rules}\n",
    "            if abs(sum(distrib[k] for k in distrib) - 1.0) > .01:\n",
    "                print(distrib)\n",
    "            rule = DictionaryProbDist(distrib).generate()\n",
    "\n",
    "            # Appends to the tree\n",
    "            tree = []\n",
    "            for node in rule.rhs():\n",
    "                if is_terminal(node):\n",
    "                    tree.append(node)\n",
    "                else:\n",
    "                    subtree = sample_from_lhs(node)\n",
    "                    tree.append(subtree)\n",
    "            return tree\n",
    "\n",
    "        S = self.grammar.start()\n",
    "        rule_index = collections.defaultdict(list)\n",
    "        for r in self.grammar.productions():\n",
    "            rule_index[r.lhs()].append(r)\n",
    "        return sample_from_lhs(S)\n",
    "\n",
    "    def serialize_tree(self, tree):\n",
    "        buf = []\n",
    "        def consume(tree):\n",
    "            if type(tree)==list:\n",
    "                for subtree in tree:\n",
    "                    consume(subtree)\n",
    "            else:\n",
    "                buf.append(tree)\n",
    "\n",
    "        consume(tree)\n",
    "        out = self.sep_char.join(buf)\n",
    "        return out\n",
    "\n",
    "    def generate_sequences(self, n_sequences, len_max=500,\n",
    "                           padded_size=None, padding_char='~'):\n",
    "        '''\n",
    "            padded_size: None means no padding\n",
    "                        -1 means padding to max sequence size\n",
    "                        some integer n means padding to size n\n",
    "                             (truncates if necessary)\n",
    "        '''\n",
    "        # Generates random sentences\n",
    "        len_max = 10000000 if len_max is None else len_max\n",
    "        out = []\n",
    "        N = 0\n",
    "        while len(out) < n_sequences:\n",
    "            tree = self.sample_tree()\n",
    "            seq = self.serialize_tree(tree)\n",
    "            if len(seq) < len_max: out.append(seq)\n",
    "            N += 1\n",
    "            assert N < 10000000\n",
    "\n",
    "        M = sum(len(s) for s in out)*1.0 / len(out)\n",
    "        #print 'Avg sequence size:', M\n",
    "        if padded_size is None:\n",
    "            return out\n",
    "\n",
    "        # Checks max size\n",
    "        if 0 <= padded_size < M:\n",
    "            warnings.warn('Padding size is smaller that longest sequence'\n",
    "                          ' I will truncate!')\n",
    "        if padded_size == -1:\n",
    "            padded_size = M\n",
    "\n",
    "        for i,s in enumerate(out):\n",
    "            if len(s) > padded_size:\n",
    "                s = s[:padded_size]\n",
    "            else:\n",
    "                s = s.rjust(padded_size, padding_char)\n",
    "            out[i] = s\n",
    "\n",
    "        return out\n",
    "\n",
    "    #########################\n",
    "    # Parsing-based Feature #\n",
    "    #########################\n",
    "    def tokenize(self, expr, skipchars=['~'], ignore_errors=False):\n",
    "        # Non teriminals\n",
    "        terminals = set(self.term_nodes())\n",
    "        L = max(len(w) for w in terminals)\n",
    "        def terminal(i, expr):\n",
    "            for wid in reversed(range(1, L+1)):\n",
    "                j = i + wid\n",
    "                if j > len(expr):\n",
    "                    continue\n",
    "                w = expr[i:j]\n",
    "                if w in terminals:\n",
    "                    return w\n",
    "            return None\n",
    "\n",
    "        tokens = []\n",
    "        index = []\n",
    "        i = 0\n",
    "        while i < len(expr):\n",
    "            if not expr[i] in skipchars:\n",
    "                term = terminal(i, expr)\n",
    "                if term is None:\n",
    "                    if not ignore_errors:\n",
    "                        raise ValueError('Could not match token', expr[i:])\n",
    "                    else:\n",
    "                        return [], []\n",
    "                tokens.append(term)\n",
    "                index.append((i,len(term)))\n",
    "                i += len(term)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens, index\n",
    "\n",
    "    def parse(self, tokens):\n",
    "        if len(tokens) == 0:\n",
    "            return []\n",
    "        parse = None\n",
    "        for p in self.parser.parse(tokens):\n",
    "            parse = p\n",
    "            break\n",
    "        return parse\n",
    "\n",
    "    # Warning: returns word level features\n",
    "    def parse_to_features(self, tree, binary=True):\n",
    "        n_tokens = len(tree.leaves())\n",
    "        nt_symbols = self.non_term_nodes()\n",
    "\n",
    "        rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "        rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "        def visit(tree, offset):\n",
    "            if isinstance(tree, Tree):\n",
    "                n_tokens = 0\n",
    "                for subtree in tree:\n",
    "                    n_tokens += visit(subtree, offset + n_tokens)\n",
    "                symb = tree.label()\n",
    "                j = rule2feat[symb]\n",
    "                if binary:\n",
    "                    rule_feats[offset:offset+n_tokens, j] = 1\n",
    "                else:\n",
    "                    rule_feats[offset:offset+n_tokens, j] += 1\n",
    "                return n_tokens\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        visit(tree, 0)\n",
    "        return rule_feats\n",
    "\n",
    "    def word_to_char_feat(self, seq, lex_index, w_feats):\n",
    "        ch_len = len(seq)\n",
    "        w_len  = w_feats.shape[0]\n",
    "        n_feats = w_feats.shape[1]\n",
    "        ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "        for i_w in range(w_len):\n",
    "            f_w  = w_feats[i_w,...]\n",
    "            i_ch,len_ch = lex_index[i_w]\n",
    "            f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "            ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "\n",
    "        return ch_feats\n",
    "\n",
    "    def extract_parse_feats(self, seq, binary=True, verbose=False):\n",
    "        # Tokenizes and parses\n",
    "        start_t = timeit.default_timer()\n",
    "        tokens, tok_index = self.tokenize(seq)\n",
    "        parse_tree = self.parse(tokens)\n",
    "        if verbose:\n",
    "            print(len(tokens))\n",
    "        # Converts to word-, then character-level features\n",
    "        w_tree_features = self.parse_to_features(parse_tree)\n",
    "        \n",
    "        tree_features = self.word_to_char_feat(seq, tok_index, w_tree_features)\n",
    "        stop_t = timeit.default_timer()\n",
    "        runtime = stop_t - start_t\n",
    "        return tree_features, runtime\n",
    "\n",
    "    def detect_bounds(self, x):\n",
    "        if len(x) < 2:\n",
    "            return x\n",
    "        x = np.int_(x)\n",
    "        x2 = np.insert(x,0,0)\n",
    "        start = x2[1:] - x2[:-1] > 0\n",
    "        x2 = np.append(x,0)\n",
    "        end = x2[:-1] - x2[1:] > 0\n",
    "        return np.logical_or(start, end)\n",
    "\n",
    "\n",
    "    def sub_parse_features(self, orig_sequences, prov_index, w_size,\n",
    "                            include_impulses=True,\n",
    "                            name_prefix=FEAT_NAME_PREFIX):\n",
    "\n",
    "        # Actual features\n",
    "        global PARSE_TIME\n",
    "        PARSE_TIME = 0\n",
    "        mem_cache = collections.OrderedDict()\n",
    "        CACHE_LEN = float('inf')\n",
    "        out = []\n",
    "        symbols = self.non_term_nodes()\n",
    "        for i_S, S in enumerate(symbols):\n",
    "\n",
    "            # Creates the feature function\n",
    "            # (default arg assignment prevent late binding)\n",
    "            def feat1(seq, i_S=i_S, S=S.symbol()):\n",
    "\n",
    "                # Retrieves mother sequence\n",
    "                i_seq, i_start, i_end  = prov_index[seq]\n",
    "\n",
    "                if i_seq in mem_cache:\n",
    "                    # If sequence in cache, retrieves it\n",
    "                    seq_feats = mem_cache[i_seq]\n",
    "\n",
    "                else:\n",
    "                    # otherwise compute tree ....\n",
    "                    moth_seq = orig_sequences[i_seq]\n",
    "                    seq_feats, runtime = self.extract_parse_feats(moth_seq)\n",
    "                    global PARSE_TIME\n",
    "                    PARSE_TIME += runtime\n",
    "\n",
    "                    # ... and cache it\n",
    "                    mem_cache[i_seq] = np.bool_(seq_feats)\n",
    "                    if len(mem_cache) > CACHE_LEN:\n",
    "                        cache.popitem(last=False)\n",
    "\n",
    "                feat_vals = seq_feats[i_start:i_end,i_S]\n",
    "\n",
    "                # If necesary, pads with zeroes\n",
    "                L = i_end - i_start\n",
    "                assert L == feat_vals.shape[0]\n",
    "                if L < w_size:\n",
    "                    tmp = np.zeros((w_size))\n",
    "                    tmp[-L:] = feat_vals\n",
    "                    feat_vals = tmp\n",
    "\n",
    "                assert feat_vals.shape[0] == len(seq)\n",
    "                feat_name = 'F_' + S\n",
    "                return feat_vals, feat_name\n",
    "\n",
    "            def feat2(seq, i_S=i_S, S=S.symbol()):\n",
    "\n",
    "                # Retrieves mother sequence\n",
    "                i_seq, i_start, i_end  = prov_index[seq]\n",
    "\n",
    "                if i_seq in mem_cache:\n",
    "                    # If sequence in cache, retrieves it\n",
    "                    seq_feats = mem_cache[i_seq]\n",
    "\n",
    "                else:\n",
    "                    # otherwise compute tree ....\n",
    "                    moth_seq = orig_sequences[i_seq]\n",
    "                    seq_feats, runtime = self.extract_parse_feats(moth_seq)\n",
    "                    global PARSE_TIME\n",
    "                    PARSE_TIME += runtime\n",
    "\n",
    "                    # ... and cache it\n",
    "                    mem_cache[i_seq] = np.bool_(seq_feats)\n",
    "                    if len(mem_cache) > CACHE_LEN:\n",
    "                        cache.popitem(last=False)\n",
    "\n",
    "                feat_vals = seq_feats[i_start:i_end,i_S]\n",
    "                feat_vals = self.detect_bounds(feat_vals)\n",
    "\n",
    "                # If necesary, pads with zeroes\n",
    "                L = i_end - i_start\n",
    "                assert L == feat_vals.shape[0]\n",
    "                if L < w_size:\n",
    "                    tmp = np.zeros((w_size))\n",
    "                    tmp[-L:] = feat_vals\n",
    "                    feat_vals = tmp\n",
    "\n",
    "                assert feat_vals.shape[0] == len(seq)\n",
    "                feat_name = 'B_' + S\n",
    "                return feat_vals, feat_name\n",
    "\n",
    "\n",
    "            if include_impulses:\n",
    "                out += [feat1,feat2]\n",
    "            else:\n",
    "                out += [feat1]\n",
    "\n",
    "        def get_parse_time():\n",
    "            return PARSE_TIME\n",
    "\n",
    "        def reset():\n",
    "            mem_cache.clear()\n",
    "            global PARSE_TIME\n",
    "            PARSE_TIME = 0\n",
    "\n",
    "        return out, get_parse_time, reset\n",
    "\n",
    "    #######################\n",
    "    # Token-based Feature #\n",
    "    #######################\n",
    "    def extract_token_feats(self, seq):\n",
    "        tokens, tok_index = self.tokenize(seq)\n",
    "        all_tokens = self.term_nodes()\n",
    "        tok2feat = {f:i for i,f in enumerate(all_tokens)}\n",
    "\n",
    "        # Generates the word-level features\n",
    "        w_feats = np.zeros((len(tokens), len(all_tokens)))\n",
    "        for i,tok in enumerate(tokens):\n",
    "            j = tok2feat[tok]\n",
    "            w_feats[i,j] = 1\n",
    "\n",
    "        # Expands to char-level features\n",
    "        feats = self.word_to_char_feat(seq, tok_index, w_feats)\n",
    "        return feats\n",
    "\n",
    "    def token_features(self, name_prefix=''):\n",
    "        # Sets buffers up\n",
    "        if hasattr(self, 'token_feats'):\n",
    "            print('Reinitializing the token-based features buffer')\n",
    "        self.token_feats = {}\n",
    "\n",
    "        out = []\n",
    "        symbols = self.term_nodes()\n",
    "        for i_S, S in enumerate(symbols):\n",
    "\n",
    "            # Creates the feature function\n",
    "            # (default arg assignment prevent late binding)\n",
    "            def feat(seq, i_S=i_S, S=S):\n",
    "                # If not buffered, process the sequence\n",
    "                if seq not in self.token_feats:\n",
    "                    self.token_feats[seq] = self.extract_token_feats(seq)\n",
    "                # Fetches the value\n",
    "                feat_vals = self.token_feats[seq][:,i_S]\n",
    "                feat_name = name_prefix + S\n",
    "                return feat_vals, feat_name\n",
    "\n",
    "            out.append(feat)\n",
    "\n",
    "        return out\n",
    "\n",
    "    ##############\n",
    "    # Test model #\n",
    "    ##############\n",
    "    def test_model(self, model, batch_size, n_batches, max_len, char2int, w_size,\n",
    "                start_chars=' SELECT', end_char='$', padding_char = '~'):\n",
    "        # Generates sentences\n",
    "        S = []\n",
    "        start_chars = start_chars.rjust(w_size, padding_char)\n",
    "        for _ in range(n_batches):\n",
    "            S += generator.generate_sentences(\n",
    "                start_chars, end_char, max_len, model, char2int, batch_size)\n",
    "        print('Generated', len(S), 'sentences')\n",
    "\n",
    "        # Parses\n",
    "        def check(s):\n",
    "            tokens, _ = self.tokenize(s,ignore_errors=True)\n",
    "            parse = self.parse(tokens)\n",
    "            if parse is not None and len(parse) > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        n_ok = 0\n",
    "        for s in S: n_ok += check(s)\n",
    "        return n_ok * 1.0 / len(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window(sequences, w_size=-1, step_size=1, \\\n",
    "                 n_test_tuples=float('Inf'), pad_char='~'):\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "    for i_seq, seq in enumerate(sequences):\n",
    "        steps = range(step_size, len(seq), step_size)\n",
    "        if len(steps) == 0:\n",
    "            steps = [len(seq)-1]\n",
    "        for i in steps:\n",
    "            end = i\n",
    "            start = max(0, i-w_size)\n",
    "            s = seq[start:end]\n",
    "            assert len(s) <= w_size\n",
    "            x = s.rjust(w_size, pad_char)\n",
    "            assert len(x) == w_size\n",
    "            X.append(x)\n",
    "            y.append(seq[i])\n",
    "            prov_index[x] = (i_seq, start, end)\n",
    "            if len(X) > n_test_tuples:\n",
    "                break\n",
    "        if len(X) > n_test_tuples:\n",
    "                break\n",
    "\n",
    "    assert len(X) == len(y)\n",
    "    X, y = adjust_to_batch(X,y)\n",
    "    print('Will use', len(X), 'sentences')\n",
    "    return X, y, prov_index, i_seq\n",
    "\n",
    "def adjust_to_batch(X, y):\n",
    "    assert len(X) == len(y)\n",
    "    N = len(X)\n",
    "    N -= N % PARAMS['batch_size']\n",
    "    assert N > 0\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    return X,y\n",
    "\n",
    "def generate_seq(grammar, n_seq, n_tuples=None, step=None, w_size=None):\n",
    "    if n_tuples is None:\n",
    "        sequences = grammar.generate_sequences(\n",
    "            n_seq, len_max=PARAMS['max_seq_size'])\n",
    "    else:\n",
    "        sequences = []\n",
    "        n_tuples_gen = 0\n",
    "        while n_tuples_gen <= n_tuples:\n",
    "            seqs = grammar.generate_sequences(\n",
    "                n_seq, len_max=PARAMS['max_seq_size'])\n",
    "            sequences += seqs\n",
    "            ntups = lambda s: max((len(s)-w_size)//n_seq, 1)\n",
    "            n_gen = sum(ntups(s) for s in seqs)\n",
    "            n_tuples_gen += n_gen\n",
    "    print ('Done. Generated a total of:', len(sequences), 'sentences')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sql_full_XL.pcfg\n",
      "Loaded grammar with 198 rules\n",
      "Grammar rules before removing zero-rules: 280\n",
      "Grammar rules after removing zero-rules: 272\n",
      "Created parser\n",
      "\n",
      "** Generates data\n",
      "Done. Generated a total of: 128 sentences\n",
      "Done. Generated a total of: 200 sentences\n",
      "Will use 14848 sentences\n",
      "Will use 128 sentences\n",
      "Truncated test to 5 queries\n"
     ]
    }
   ],
   "source": [
    "grammar = Grammar(pcfg_file='sql_full_XL.pcfg')\n",
    "n_rules = grammar.n_rules()\n",
    "n_feats = grammar.n_features()\n",
    "\n",
    "# Generates original sequences\n",
    "print('\\n** Generates data')\n",
    "# Fixed seed for repeatability\n",
    "random.seed(55555)\n",
    "n_seq_test = PARAMS[\"n_seq_test\"]\n",
    "n_test_tuples = PARAMS[\"n_tup_test\"][0]\n",
    "\n",
    "train_sequences = generate_seq(grammar, PARAMS['n_seq_train'])\n",
    "test_sequences = generate_seq(grammar, n_seq_test, \\\n",
    "                                n_tuples=n_test_tuples,\n",
    "                                step=PARAMS['test_step'],\n",
    "                                w_size=PARAMS['window_size'])\n",
    "\n",
    "train_from, train_to, _, _ = \\\n",
    "    slide_window(train_sequences, PARAMS['window_size'])\n",
    "test_from, test_to, prov_index, last_seq = \\\n",
    "    slide_window(test_sequences, PARAMS['window_size'],\n",
    "                 PARAMS['test_step'], n_test_tuples=n_test_tuples)\n",
    "\n",
    "# Adjustes n test seqs\n",
    "test_sequences = test_sequences[:last_seq+1]\n",
    "print('Truncated test to', len(test_sequences), 'queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prov_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS['window_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(sequences):\n",
    "    chars = set()\n",
    "    for s in sequences:\n",
    "        for c in s:\n",
    "            chars.add(c)\n",
    "    return {c:i for i,c in enumerate(list(chars))}\n",
    "\n",
    "char2int = make_dict(train_from + test_from+train_to +test_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    " '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    " '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    " 'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    " 'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    " 'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "'t': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X matrices: (14848, 30, 73) (128, 30, 73)\n",
      "y matrices: (14848, 73) (128, 73)\n"
     ]
    }
   ],
   "source": [
    "class TwoDimEncoders:\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_encoded(seqs, seqs2=None, cust_char2int=None):\n",
    "\n",
    "        if cust_char2int is None:\n",
    "            all_chars = reduce(lambda chars,seq : set(chars) | set(seq), seqs)\n",
    "            if seqs2 is not None:\n",
    "                all_chars |= reduce(lambda chars,seq : set(chars) | set(seq),\n",
    "                                    seqs2)\n",
    "            all_chars = sorted(all_chars)\n",
    "\n",
    "            char2int =  {c:i for i, c in enumerate(all_chars)}\n",
    "            print \"Total vocabulary len_sequence: \", len(all_chars)\n",
    "        else:\n",
    "            char2int = cust_char2int\n",
    "\n",
    "        encoded_seqs = [[char2int[x] for x in seq] for seq in seqs]\n",
    "        if seqs2 is None:\n",
    "            return encoded_seqs, char2int\n",
    "\n",
    "        encoded_seqs2 = [[char2int[x] for x in seq] for seq in seqs2]\n",
    "        return encoded_seqs, char2int, encoded_seqs2\n",
    "\n",
    "    @staticmethod\n",
    "    def encoded_to_bin_tensor(enc_seqs, char2int, enc_seqs2=None,\n",
    "                                start_at_min=False):\n",
    "\n",
    "        min_x = min(char2int.values()) if start_at_min else 0\n",
    "        n_chars = max(char2int.values()) - min_x + 1\n",
    "\n",
    "        seq_len = len(enc_seqs[0])\n",
    "\n",
    "        X = np.zeros((len(enc_seqs), seq_len, n_chars), dtype=np.int)\n",
    "        for i, enc_seq in enumerate(enc_seqs):\n",
    "            for j, x in enumerate(enc_seq):\n",
    "                k = x - min_x\n",
    "                X[i, j, k] = 1\n",
    "\n",
    "        if enc_seqs2 is None:\n",
    "            return X\n",
    "\n",
    "        X2 = np.zeros((len(enc_seqs2), seq_len, n_chars), dtype=np.int)\n",
    "        for i, enc_seq in enumerate(enc_seqs2):\n",
    "            for j, x in enumerate(enc_seq):\n",
    "                k = x - min_x\n",
    "                X2[i, j, k] = 1\n",
    "\n",
    "        return X, X2\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_bin_tensor(seqs, seqs2=None, cust_char2int=None):\n",
    "        encoding_out = TwoDimEncoders.raw_to_encoded(seqs, seqs2, cust_char2int)\n",
    "        if len(encoding_out) == 2:\n",
    "            encoded_seqs, char2int = encoding_out\n",
    "            encoded_seqs2 = None\n",
    "        else:\n",
    "            encoded_seqs, char2int, encoded_seqs2 = encoding_out\n",
    "\n",
    "        bin_tensors = TwoDimEncoders.encoded_to_bin_tensor(\n",
    "            encoded_seqs, char2int, encoded_seqs2)\n",
    "\n",
    "        if type(bin_tensors) is not tuple:\n",
    "            return bin_tensors, char2int\n",
    "\n",
    "        out = bin_tensors + (char2int,)\n",
    "        return out\n",
    "\n",
    "X_train, X_test, char2int = \\\n",
    "    TwoDimEncoders.raw_to_bin_tensor(\\\n",
    "        train_from, test_from, cust_char2int=char2int)\n",
    "    \n",
    "print('X matrices:', X_train.shape, X_test.shape)\n",
    "\n",
    "class OneDimEncoders:\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_encoded(seq, seq2=None, cust_char2int=None):\n",
    "\n",
    "        if cust_char2int is None:\n",
    "            all_chars = set(seq)\n",
    "            if seq2 is not None:\n",
    "                all_chars |= set(seq2)\n",
    "            all_chars = sorted(all_chars)\n",
    "\n",
    "            char2int =  {c:i for i, c in enumerate(all_chars)}\n",
    "        else:\n",
    "            char2int = cust_char2int\n",
    "\n",
    "        encoded_seq = [char2int[x] for x in seq]\n",
    "        if seq2 is None:\n",
    "            return encoded_seq, char2int\n",
    "\n",
    "        encoded_seq2 = [char2int[x] for x in seq2]\n",
    "        return encoded_seq, char2int, encoded_seq2\n",
    "\n",
    "    @staticmethod\n",
    "    def encoded_to_bin_tensor(enc_seq, char2int, enc_seq2=None,\n",
    "                                start_at_min=False, add_dim=False):\n",
    "\n",
    "        min_x = min(char2int.values()) if start_at_min else 0\n",
    "        n_chars = max(char2int.values()) - min_x + 1\n",
    "\n",
    "        X = np.zeros((len(enc_seq), n_chars), dtype=np.int)\n",
    "        for i, x in enumerate(enc_seq):\n",
    "            j = x - min_x\n",
    "            X[i, j] = 1\n",
    "\n",
    "        if add_dim:\n",
    "            X = X[:,np.newaxis,:]\n",
    "\n",
    "        if enc_seq2 is None:\n",
    "            return X\n",
    "\n",
    "        X2 = np.zeros((len(enc_seq2), n_chars), dtype=np.int)\n",
    "        for i, x in enumerate(enc_seq2):\n",
    "            j = x - min_x\n",
    "            X2[i, j] = 1\n",
    "\n",
    "        if add_dim:\n",
    "            X2 = X2[:,np.newaxis,:]\n",
    "\n",
    "        return X, X2\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_bin_tensor(seq, seq2=None, cust_char2int=None, add_dim=False):\n",
    "        encoding_out = OneDimEncoders.raw_to_encoded(seq, seq2, cust_char2int)\n",
    "        if len(encoding_out) == 2:\n",
    "            encoded_seq, char2int = encoding_out\n",
    "            encoded_seq2 = None\n",
    "        else:\n",
    "            encoded_seq, char2int, encoded_seq2 = encoding_out\n",
    "\n",
    "        bin_tensors = OneDimEncoders.encoded_to_bin_tensor(\n",
    "            encoded_seq, char2int, encoded_seq2, add_dim=add_dim)\n",
    "\n",
    "        if type(bin_tensors) is not tuple:\n",
    "            return bin_tensors, char2int\n",
    "\n",
    "        out = bin_tensors + (char2int,)\n",
    "        return out\n",
    "\n",
    "    \n",
    "y_train, y_test, char2int = \\\n",
    "    OneDimEncoders.raw_to_bin_tensor(\\\n",
    "        train_to, test_to, cust_char2int=char2int)\n",
    "print('y matrices:', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM,Lambda,Input\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from keras.models import load_model\n",
    "from os import path\n",
    "\n",
    "\n",
    "def vanilla_LSTM(X, y, n_states, n_layers = 1):\n",
    "    in_dim = X.shape[1:]\n",
    "    out_dim = y.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_states, return_sequences=True,input_shape=in_dim))\n",
    "    for _ in range(n_layers - 1):\n",
    "        model.add(LSTM(n_states, return_sequences=True))\n",
    "    model.add(Lambda(lambda x: x[:,-1, :]))\n",
    "    model.add(Dense(out_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_or_fit(cache, model, X_train, y_train,\n",
    "                batch_size, n_epochs, validation_split, patience=5):\n",
    "\n",
    "    if path.isfile(cache):\n",
    "        model = load_model(cache)\n",
    "    else:\n",
    "        fit_model(cache, model, X_train, y_train,\n",
    "            batch_size, n_epochs, validation_split, patience)\n",
    "    return model\n",
    "\n",
    "def fit_model(cache, model, X_train, y_train,\n",
    "              batch_size, n_epochs, validation_split, patience=5):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    checkpointer = ModelCheckpoint(filepath=cache, monitor='val_loss',\n",
    "                                   verbose=1, save_best_only=True)\n",
    "    model.fit(X_train, y_train,\n",
    "                batch_size       = batch_size,\n",
    "                epochs           = n_epochs,\n",
    "                validation_split = validation_split,\n",
    "                callbacks        = [early_stopping, checkpointer],\n",
    "                verbose          = 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14848, 30, 73)\n",
      "(14848, 73)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM,Lambda,Input\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "    \n",
    "model = vanilla_LSTM(X_train, y_train,  PARAMS['n_units'][0],  PARAMS['n_layers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 13363 samples, validate on 1485 samples\n",
      "Epoch 1/3\n",
      "13363/13363 [==============================] - 25s 2ms/step - loss: 3.8388 - acc: 0.1736 - val_loss: 3.6418 - val_acc: 0.1879\n",
      "Epoch 2/3\n",
      "13363/13363 [==============================] - 22s 2ms/step - loss: 3.4936 - acc: 0.2066 - val_loss: 3.2450 - val_acc: 0.2337\n",
      "Epoch 3/3\n",
      "13363/13363 [==============================] - 22s 2ms/step - loss: 3.0421 - acc: 0.3048 - val_loss: 2.7783 - val_acc: 0.3825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9d404fd828>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "import os\n",
    "if not os.path.exists('track_history'):\n",
    "    os.makedirs('track_history')\n",
    "\n",
    "model.save('track_history/zero_model.h5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='track_history/models-{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                               monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "        batch_size       = PARAMS['batch_size'],\n",
    "        epochs           = 3,\n",
    "        validation_split = 0.1,\n",
    "        callbacks        = [early_stopping, checkpointer],\n",
    "        verbose          = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14848"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.55200705e-05, 5.96504658e-04, 7.01373792e-05, ...,\n",
       "        1.72621862e-03, 8.19365887e-05, 2.43108720e-03],\n",
       "       [2.32807161e-05, 4.36387578e-04, 7.95411615e-05, ...,\n",
       "        2.11861986e-03, 6.47683337e-05, 4.47439263e-03],\n",
       "       [2.86140730e-05, 2.91232689e-04, 1.32407222e-04, ...,\n",
       "        4.20412235e-03, 7.59399045e-05, 1.02809006e-02],\n",
       "       ...,\n",
       "       [5.63758767e-05, 5.41349547e-03, 1.03741422e-01, ...,\n",
       "        1.92843180e-03, 1.04891807e-02, 7.82646686e-02],\n",
       "       [5.72857643e-05, 6.55220915e-03, 1.04130030e-01, ...,\n",
       "        1.79147313e-03, 1.03560546e-02, 9.09508541e-02],\n",
       "       [7.30799075e-05, 8.02156795e-03, 8.57042000e-02, ...,\n",
       "        1.99020025e-03, 8.93536024e-03, 1.02089636e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[1:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.3359375\n"
     ]
    }
   ],
   "source": [
    "def test_models(models, X_test, y_test, gram, char2int):\n",
    "    for i,model in enumerate(models):\n",
    "        model.reset_states()\n",
    "        score = model.evaluate(X_test, y_test,\n",
    "                               batch_size=PARAMS['batch_size'],\n",
    "                               verbose=2)\n",
    "        print('Test accuracy:', score[1])\n",
    "        \n",
    "\n",
    "test_models([model], X_test, y_test, grammar, char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sql_full_XL.pcfg\n",
      "Loaded grammar with 198 rules\n",
      "Grammar rules before removing zero-rules: 280\n",
      "Grammar rules after removing zero-rules: 272\n",
      "Created parser\n"
     ]
    }
   ],
   "source": [
    "grammar = Grammar(pcfg_file='sql_full_XL.pcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg_string = None\n",
    "with open('sql_full_XL.pcfg', 'r') as f:\n",
    "    pcfg_string = f.read()\n",
    "g = PCFG.fromstring(pcfg_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 198 productions>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [r for r in grammar.productions() if r.prob() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_index = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rules:\n",
    "    rule_index[r.lhs()].append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rule_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = grammar.sub_parse_features(test_sequences, prov_index, PARAMS['window_size'], include_impulses=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=0, S='symb_OUTER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=1, S='symb_FULL')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=2, S='symb_RIGHT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=3, S='symb_LEFT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=4, S='symb_INNER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=5, S='symb_NOT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=6, S='OUTER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=7, S='FULL')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=8, S='RIGHT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=9, S='LEFT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=10, S='symb_JOIN')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=11, S='INNER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=12, S='NOT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=13, S='symb_*')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=14, S='symb_-')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=15, S='symb_+')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=16, S='symb_/')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=17, S='symb_HAVING')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=18, S='str')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=19, S=\"symb_'\")>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=20, S='symb_true')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=21, S='symb_false')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=22, S='arg_list')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=23, S='fname')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=24, S='symb_ON')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=25, S='b_outer')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=26, S='outer_type')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=27, S='JOIN')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=28, S='b_inner')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=29, S='lb')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=30, S='symb_(')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=31, S='b_not')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=32, S='binaryop')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=33, S='chn')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=34, S='symb_h')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=35, S='symb_r')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=36, S='symb_w')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=37, S='symb_f')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=38, S='symb_o')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=39, S='symb_u')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=40, S='symb_c')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=41, S='symb_z')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=42, S='symb_q')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=43, S='symb_v')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=44, S='symb_x')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=45, S='symb_j')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=46, S='symb_m')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=47, S='symb_a')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=48, S='symb_e')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=49, S='symb_g')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=50, S='symb_y')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=51, S='symb_l')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=52, S='symb_n')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=53, S='symb_p')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=54, S='symb_i')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=55, S='symb_s')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=56, S='symb_t')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=57, S='symb_d')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=58, S='symb_k')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=59, S='symb_b')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=60, S='HAVING')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=61, S='string')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=62, S='boolean')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=63, S='function')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=64, S='parenval')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=65, S='number')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=66, S='symb_<=')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=67, S='symb_>=')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=68, S='symb_<>')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=69, S='symb_<')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=70, S='symb_=')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=71, S='symb_>')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=72, S='symb_OR')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=73, S='symb_AND')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=74, S='join_pred')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=75, S='ON')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=76, S='join_type')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=77, S='source_subq')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=78, S='source_table')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=79, S='symb_AS')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=80, S='biexpr')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=81, S='symb_DESC')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=82, S='symb_ASC')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=83, S='mix')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=84, S='chr')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=85, S='having_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=86, S='grouping_term')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=87, S='symb_GROUP')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=88, S='symb_WHERE')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=89, S='value')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=90, S='comp')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=91, S='OR')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=92, S='AND')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=93, S='implicit_join')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=94, S='explicit_join')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=95, S='single_source')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=96, S='symb_FROM')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=97, S='name')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=98, S='AS')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=99, S='expr')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=100, S='symb_.*')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=101, S='symb_1')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=102, S='symb_8')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=103, S='symb_4')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=104, S='symb_3')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=105, S='symb_9')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=106, S='symb_5')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=107, S='symb_2')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=108, S='symb_0')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=109, S='symb_7')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=110, S='symb_6')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=111, S='DESC')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=112, S='ASC')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=113, S='column_name')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=114, S='symb_.')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=115, S='table_name')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=116, S='b_having_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=117, S='group_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=118, S='GROUP')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=119, S='WHERE')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=120, S='pred')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=121, S='log')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=122, S='join_source')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=123, S='FROM')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=124, S='sel_res_col')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=125, S='sel_res_all_star')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=126, S='sel_res_val')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=127, S='sel_res_tab_star')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=128, S='symb_DISTINCT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=129, S='symb_OFFSET')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=130, S='dig')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=131, S='symb_LIMIT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=132, S='b_order')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=133, S='col_ref')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=134, S='symb_BY')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=135, S='symb_ORDER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=136, S='symb_INTERSECT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=137, S='gb_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=138, S='where_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=139, S='from_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=140, S='symb_,')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=141, S='select_result')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=142, S='DISTINCT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=143, S='symb_SELECT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=144, S='OFFSET')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=145, S='int')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=146, S='LIMIT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=147, S='symb_)')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=148, S='ordering_term')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=149, S='BY')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=150, S='ORDER')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=151, S='symb_union')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=152, S='symb_UNION')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=153, S='INTERSECT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=154, S='b_gb_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=155, S='b_where_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=156, S='b_from_clause')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=157, S='select_results')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=158, S='b_distinct')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=159, S='SELECT')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=160, S='limit')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=161, S='orderby')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=162, S='compound_op')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=163, S='ws')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=164, S='select_core')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=165, S='b_limit')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=166, S='b_orderby')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=167, S='select_cores')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=168, S='symb_$')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=169, S='query')>,\n",
       " <function __main__.Grammar.sub_parse_features.<locals>.feat1(seq, i_S=170, S='sequence')>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = grammar.non_term_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'symb_OUTER'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0].symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 115, 145)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prov_index[test_from[127]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sql_full_XL.pcfg\n",
      "Loaded grammar with 198 rules\n",
      "Grammar rules before removing zero-rules: 280\n",
      "Grammar rules after removing zero-rules: 272\n",
      "Created parser\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "grammar = Grammar(pcfg_file='sql_full_XL.pcfg')\n",
    "a,b = grammar.extract_parse_feats(test_sequences[0],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 171)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51073052799984"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' )  SELECT pqohm.nl65p  AS bpx': (0, 20, 50),\n",
       " ' AND ..false..<>.fj. ORDER BY ': (0, 80, 110),\n",
       " ' AS u9z.* xvl9 AS y7 WHERE g2m': (2, 20, 50),\n",
       " ' AS zlall FULL JOIN rehgis AS ': (4, 65, 95),\n",
       " ' FROM )  SELECT pqohm.nl65p  A': (0, 15, 45),\n",
       " ' JOIN l5  ON q3<=to4uao.tc WHE': (0, 210, 240),\n",
       " ' ORDER BY ff DESC ) AS r08z un': (0, 100, 130),\n",
       " ' SELECT *, ix.* FROM )  SELECT': (0, 0, 30),\n",
       " ' SELECT 38.22300, opbu.*, tlf.': (1, 0, 30),\n",
       " ' SELECT ad0 FROM e2l AS u9z.* ': (2, 0, 30),\n",
       " ' SELECT mwfl.* ORDER BY v1d.rk': (3, 0, 30),\n",
       " ' SELECT zo.v7ljvk, bxs3.etz  A': (4, 0, 30),\n",
       " ' b0.ssey  AS ydfygj FROM d4 AS': (1, 45, 75),\n",
       " ' ix.* FROM )  SELECT pqohm.nl6': (0, 10, 40),\n",
       " ' l5  ON q3<=to4uao.tc WHERE f9': (0, 215, 245),\n",
       " ' nufxa WHERE cn6f.ssi4<=71.070': (1, 75, 105),\n",
       " ' pqohm.nl65p  AS bpxhg FROM yc': (0, 30, 60),\n",
       " ' q36 AS w2orpr INNER JOIN l5  ': (0, 190, 220),\n",
       " ' t8a5gu, false, wtl.* FROM vz ': (0, 150, 180),\n",
       " ' tlf.z03n2w, i6fc.*, b0.ssey  ': (1, 25, 55),\n",
       " ' w61.j0sc<>lmvn6 WHERE 01270=l': (4, 105, 135),\n",
       " ' wtl.* FROM vz AS rk4j6.* q36 ': (0, 165, 195),\n",
       " '* FROM vz AS rk4j6.* q36 AS w2': (0, 170, 200),\n",
       " ', fks8.pvdi9a  AS oiwo FROM su': (4, 35, 65),\n",
       " '..false..<>.fj. ORDER BY ff DE': (0, 85, 115),\n",
       " '.22300, opbu.*, tlf.z03n2w, i6': (1, 10, 40),\n",
       " '.v7ljvk, bxs3.etz  AS vze, fks': (4, 10, 40),\n",
       " '0 FROM e2l AS u9z.* xvl9 AS y7': (2, 10, 40),\n",
       " '0, opbu.*, tlf.z03n2w, i6fc.*,': (1, 15, 45),\n",
       " '11 WHERE p65.bn7<oo  AND ..fal': (0, 60, 90),\n",
       " '4j6.* q36 AS w2orpr INNER JOIN': (0, 185, 215),\n",
       " '5p  AS bpxhg FROM yc11 WHERE p': (0, 40, 70),\n",
       " '6 WHERE 01270=l0d.oh ORDER BY ': (4, 120, 150),\n",
       " '65.bn7<oo  AND ..false..<>.fj.': (0, 70, 100),\n",
       " '6f.ssi4<=71.07001 LIMIT 8519 O': (1, 90, 120),\n",
       " '75. * t8a5gu, false, wtl.* FRO': (0, 145, 175),\n",
       " '7<oo  AND ..false..<>.fj. ORDE': (0, 75, 105),\n",
       " '8.pvdi9a  AS oiwo FROM su AS z': (4, 40, 70),\n",
       " '8z union  SELECT .1875. * t8a5': (0, 125, 155),\n",
       " '9z.* xvl9 AS y7 WHERE g2mti8.m': (2, 25, 55),\n",
       " '<=to4uao.tc WHERE f9c7kr<=azf8': (0, 225, 255),\n",
       " '>.fj. ORDER BY ff DESC ) AS r0': (0, 95, 125),\n",
       " '>lmvn6 WHERE 01270=l0d.oh ORDE': (4, 115, 145),\n",
       " 'AS oiwo FROM su AS zlall FULL ': (4, 50, 80),\n",
       " 'AS r08z union  SELECT .1875. *': (0, 120, 150),\n",
       " 'AS rk4j6.* q36 AS w2orpr INNER': (0, 180, 210),\n",
       " 'AS w2orpr INNER JOIN l5  ON q3': (0, 195, 225),\n",
       " 'AS ydfygj FROM d4 AS nufxa WHE': (1, 55, 85),\n",
       " 'CT *, ix.* FROM )  SELECT pqoh': (0, 5, 35),\n",
       " 'CT 38.22300, opbu.*, tlf.z03n2': (1, 5, 35),\n",
       " 'CT ad0 FROM e2l AS u9z.* xvl9 ': (2, 5, 35),\n",
       " 'CT mwfl.* ORDER BY v1d.rk ASC ': (3, 5, 35),\n",
       " 'CT zo.v7ljvk, bxs3.etz  AS vze': (4, 5, 35),\n",
       " 'ELECT pqohm.nl65p  AS bpxhg FR': (0, 25, 55),\n",
       " 'ERE p65.bn7<oo  AND ..false..<': (0, 65, 95),\n",
       " 'FROM d4 AS nufxa WHERE cn6f.ss': (1, 65, 95),\n",
       " 'FULL JOIN rehgis AS k8hjvc  ON': (4, 75, 105),\n",
       " 'INNER JOIN l5  ON q3<=to4uao.t': (0, 205, 235),\n",
       " 'JOIN rehgis AS k8hjvc  ON w61.': (4, 80, 110),\n",
       " 'M e2l AS u9z.* xvl9 AS y7 WHER': (2, 15, 45),\n",
       " 'M vz AS rk4j6.* q36 AS w2orpr ': (0, 175, 205),\n",
       " 'OM su AS zlall FULL JOIN rehgi': (4, 60, 90),\n",
       " 'OM yc11 WHERE p65.bn7<oo  AND ': (0, 55, 85),\n",
       " 'ON q3<=to4uao.tc WHERE f9c7kr<': (0, 220, 250),\n",
       " 'R BY ff DESC ) AS r08z union  ': (0, 105, 135),\n",
       " 'RE cn6f.ssi4<=71.07001 LIMIT 8': (1, 85, 115),\n",
       " 'RE f9c7kr<=azf8x GROUP BY  in7': (0, 240, 270),\n",
       " 'S bpxhg FROM yc11 WHERE p65.bn': (0, 45, 75),\n",
       " 'S vze, fks8.pvdi9a  AS oiwo FR': (4, 30, 60),\n",
       " 'SC ) AS r08z union  SELECT .18': (0, 115, 145),\n",
       " 'SELECT .1875. * t8a5gu, false,': (0, 135, 165),\n",
       " 'T .1875. * t8a5gu, false, wtl.': (0, 140, 170),\n",
       " 'a WHERE cn6f.ssi4<=71.07001 LI': (1, 80, 110),\n",
       " 'alse, wtl.* FROM vz AS rk4j6.*': (0, 160, 190),\n",
       " 'bu.*, tlf.z03n2w, i6fc.*, b0.s': (1, 20, 50),\n",
       " 'c  ON w61.j0sc<>lmvn6 WHERE 01': (4, 100, 130),\n",
       " 'c WHERE f9c7kr<=azf8x GROUP BY': (0, 235, 265),\n",
       " 'd4 AS nufxa WHERE cn6f.ssi4<=7': (1, 70, 100),\n",
       " 'fc.*, b0.ssey  AS ydfygj FROM ': (1, 40, 70),\n",
       " 'ff DESC ) AS r08z union  SELEC': (0, 110, 140),\n",
       " 'fl.* ORDER BY v1d.rk ASC ) o3p': (3, 10, 40),\n",
       " 'fygj FROM d4 AS nufxa WHERE cn': (1, 60, 90),\n",
       " 'gu, false, wtl.* FROM vz AS rk': (0, 155, 185),\n",
       " 'hg FROM yc11 WHERE p65.bn7<oo ': (0, 50, 80),\n",
       " 'i4<=71.07001 LIMIT 8519 OFFSET': (1, 95, 125),\n",
       " 'i9a  AS oiwo FROM su AS zlall ': (4, 45, 75),\n",
       " 'ion  SELECT .1875. * t8a5gu, f': (0, 130, 160),\n",
       " 'j0sc<>lmvn6 WHERE 01270=l0d.oh': (4, 110, 140),\n",
       " 'k8hjvc  ON w61.j0sc<>lmvn6 WHE': (4, 95, 125),\n",
       " 'lall FULL JOIN rehgis AS k8hjv': (4, 70, 100),\n",
       " 'm.nl65p  AS bpxhg FROM yc11 WH': (0, 35, 65),\n",
       " 'orpr INNER JOIN l5  ON q3<=to4': (0, 200, 230),\n",
       " 'rehgis AS k8hjvc  ON w61.j0sc<': (4, 85, 115),\n",
       " 's AS k8hjvc  ON w61.j0sc<>lmvn': (4, 90, 120),\n",
       " 'se..<>.fj. ORDER BY ff DESC ) ': (0, 90, 120),\n",
       " 'sey  AS ydfygj FROM d4 AS nufx': (1, 50, 80),\n",
       " 'tz  AS vze, fks8.pvdi9a  AS oi': (4, 25, 55),\n",
       " 'uao.tc WHERE f9c7kr<=azf8x GRO': (0, 230, 260),\n",
       " 'vk, bxs3.etz  AS vze, fks8.pvd': (4, 15, 45),\n",
       " 'w, i6fc.*, b0.ssey  AS ydfygj ': (1, 35, 65),\n",
       " 'wo FROM su AS zlall FULL JOIN ': (4, 55, 85),\n",
       " 'xs3.etz  AS vze, fks8.pvdi9a  ': (4, 20, 50),\n",
       " 'xvl9 AS y7 WHERE g2mti8.mt00sr': (2, 30, 60),\n",
       " 'z03n2w, i6fc.*, b0.ssey  AS yd': (1, 30, 60),\n",
       " '~~~~~ SELECT *, ix.* FROM )  S': (0, 0, 25),\n",
       " '~~~~~ SELECT 38.22300, opbu.*,': (1, 0, 25),\n",
       " '~~~~~ SELECT ad0 FROM e2l AS u': (2, 0, 25),\n",
       " '~~~~~ SELECT mwfl.* ORDER BY v': (3, 0, 25),\n",
       " '~~~~~ SELECT zo.v7ljvk, bxs3.e': (4, 0, 25),\n",
       " '~~~~~~~~~~ SELECT *, ix.* FROM': (0, 0, 20),\n",
       " '~~~~~~~~~~ SELECT 38.22300, op': (1, 0, 20),\n",
       " '~~~~~~~~~~ SELECT ad0 FROM e2l': (2, 0, 20),\n",
       " '~~~~~~~~~~ SELECT mwfl.* ORDER': (3, 0, 20),\n",
       " '~~~~~~~~~~ SELECT zo.v7ljvk, b': (4, 0, 20),\n",
       " '~~~~~~~~~~~~~~~ SELECT *, ix.*': (0, 0, 15),\n",
       " '~~~~~~~~~~~~~~~ SELECT 38.2230': (1, 0, 15),\n",
       " '~~~~~~~~~~~~~~~ SELECT ad0 FRO': (2, 0, 15),\n",
       " '~~~~~~~~~~~~~~~ SELECT mwfl.* ': (3, 0, 15),\n",
       " '~~~~~~~~~~~~~~~ SELECT zo.v7lj': (4, 0, 15),\n",
       " '~~~~~~~~~~~~~~~~~~~~ SELECT *,': (0, 0, 10),\n",
       " '~~~~~~~~~~~~~~~~~~~~ SELECT 38': (1, 0, 10),\n",
       " '~~~~~~~~~~~~~~~~~~~~ SELECT ad': (2, 0, 10),\n",
       " '~~~~~~~~~~~~~~~~~~~~ SELECT mw': (3, 0, 10),\n",
       " '~~~~~~~~~~~~~~~~~~~~ SELECT zo': (4, 0, 10),\n",
       " '~~~~~~~~~~~~~~~~~~~~~~~~~ SELE': (4, 0, 5)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prov_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:30,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 30, 128)           103424    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 73)                9417      \n",
      "=================================================================\n",
      "Total params: 112,841\n",
      "Trainable params: 112,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "outputs = [model.layers[l].output for l in [0]]\n",
    "spymodel = Model(inputs = model.input, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 30, 128)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spymodel.predict(X_test[1:52]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 30, 73)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1:128].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sqldata.txt', 'w') as f:\n",
    "    for item in test_sequences:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('sqldata.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        lines.append(line.split(\"\\n\")[0])\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines == test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import is_terminal, is_nonterminal\n",
    "term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "terminals = set(term_nodes)\n",
    "L = max(len(w) for w in terminals)\n",
    "def terminal(i, expr):\n",
    "    for wid in reversed(range(1, L+1)):\n",
    "        j = i + wid\n",
    "        if j > len(expr):\n",
    "            continue\n",
    "        w = expr[i:j]\n",
    "        if w in terminals:\n",
    "            return w\n",
    "    return None\n",
    "\n",
    "tokens = []\n",
    "index = []\n",
    "i = 0\n",
    "skipchars=['~']\n",
    "while i < len(expr):\n",
    "    if not expr[i] in skipchars:\n",
    "        term = terminal(i, expr)\n",
    "        if term is None:\n",
    "            if not ignore_errors:\n",
    "                raise ValueError('Could not match token', expr[i:])\n",
    "            else:\n",
    "                print(\"null\")\n",
    "        tokens.append(term)\n",
    "        index.append((i,len(term)))\n",
    "        i += len(term)\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 272 productions>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "temmp = symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = test_sequences[0]\n",
    "def generate_parsetree(inputdata):\n",
    "    import warnings\n",
    "    import copy\n",
    "    import collections\n",
    "    import time, timeit\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    from nltk import Tree\n",
    "    from nltk import PCFG,CFG\n",
    "    from nltk.grammar import is_terminal, is_nonterminal, ProbabilisticProduction, Nonterminal\n",
    "    from nltk.probability import DictionaryProbDist\n",
    "    expr = inputdata\n",
    "    gram = None\n",
    "    with open('sql_full_XL.pcfg', 'r') as f:\n",
    "        pcfg_string = f.read()\n",
    "        from nltk import PCFG\n",
    "        gram = PCFG.fromstring(pcfg_string)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    ix = rule_index\n",
    "    len_ix = {lhs:len(rules) for lhs,rules in ix.items()}\n",
    "    counter = 0\n",
    "    term_index = {}\n",
    "    new_rules = []\n",
    "    for rule in gram.productions():\n",
    "        # If Nonterm := Term or Nontern := Nonterm rule, skip\n",
    "        if len_ix[rule.lhs()]==1 and len(rule.rhs())==1:\n",
    "            new_rules.append(rule)\n",
    "            continue\n",
    "        # Otherwise creates a new rule\n",
    "        new_rhs = []\n",
    "        for r in rule.rhs():\n",
    "            if is_nonterminal(r) :\n",
    "                new_rhs.append(r)\n",
    "            else:\n",
    "                if r not in term_index:\n",
    "                    new_left = Nonterminal('symb_'+str(r))\n",
    "                    prule = ProbabilisticProduction(new_left,\n",
    "                                                    [r],\n",
    "                                                    prob=1.0)\n",
    "                    term_index[r]=prule\n",
    "                new_rhs.append(new_left)\n",
    "        new_rule = ProbabilisticProduction(rule.lhs(),\n",
    "                                           new_rhs,\n",
    "                                           prob=rule.prob())\n",
    "        new_rules.append(new_rule)\n",
    "    new_rules += term_index.values()\n",
    "    gram = PCFG(gram.start(), new_rules)\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    old_grammar = gram\n",
    "    # BFS\n",
    "    arules = set()\n",
    "    visited = set()\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            arules.add(r)\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.add(symb)\n",
    "    # Creates new rules\n",
    "    arules = list(arules)\n",
    "    check_rules(arules)\n",
    "    gram = PCFG(old_grammar.start(), arules)\n",
    "\n",
    "    symbols = set()\n",
    "    for rule in gram.productions():\n",
    "        symbols.add(rule.lhs())\n",
    "        for r in rule.rhs():\n",
    "            symbols.add(r)\n",
    "\n",
    "    \n",
    "    term_nodes = [S for S in symbols if is_terminal(S)]\n",
    "    terminals = set(term_nodes)\n",
    "    L = max(len(w) for w in terminals)\n",
    "    def terminal(i, expr):\n",
    "        for wid in reversed(range(1, L+1)):\n",
    "            j = i + wid\n",
    "            if j > len(expr):\n",
    "                continue\n",
    "            w = expr[i:j]\n",
    "            if w in terminals:\n",
    "                return w\n",
    "        return None\n",
    "\n",
    "    tokens = []\n",
    "    index = []\n",
    "    i = 0\n",
    "    skipchars=['~']\n",
    "    while i < len(expr):\n",
    "        if not expr[i] in skipchars:\n",
    "            term = terminal(i, expr)\n",
    "            if term is None:\n",
    "                if not ignore_errors:\n",
    "                    raise ValueError('Could not match token', expr[i:])\n",
    "                else:\n",
    "                    print(\"null\")\n",
    "            tokens.append(term)\n",
    "            index.append((i,len(term)))\n",
    "            i += len(term)\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    parse = None\n",
    "    for p in parser.parse(tokens):\n",
    "        parse = p\n",
    "        break\n",
    "\n",
    "    rules = [r for r in gram.productions() if r.prob() > 0]\n",
    "    rule_index = collections.defaultdict(list)\n",
    "    for r in rules:\n",
    "        rule_index[r.lhs()].append(r)\n",
    "    # Gets the symbols BFS\n",
    "    visited = [gram.start()]\n",
    "    Q = collections.deque()\n",
    "    Q.append(gram.start())\n",
    "\n",
    "    while len(Q) > 0:\n",
    "        # Gets first symbol in queue\n",
    "        symb = Q.popleft()\n",
    "        # Fetches corresponding rules\n",
    "        rules = rule_index[symb]\n",
    "        for r in rules:\n",
    "            for symb in r.rhs():\n",
    "                if is_nonterminal(symb) and not symb in visited:\n",
    "                    Q.append(symb)\n",
    "                    visited.append(symb)\n",
    "\n",
    "    visited = list(reversed(visited))\n",
    "\n",
    "    # Alternative method for checking correctness\n",
    "    alt_visited = [S for S in symbols if is_nonterminal(S)]\n",
    "    assert set(visited) == set(alt_visited)\n",
    "\n",
    "    non_term_nodes = visited\n",
    "    tree = parse\n",
    "    n_tokens = len(tree.leaves())\n",
    "    nt_symbols = non_term_nodes\n",
    "\n",
    "    rule_feats = np.zeros((n_tokens, len(nt_symbols)))\n",
    "    rule2feat = {s.symbol():i for i,s in enumerate(nt_symbols)}\n",
    "\n",
    "    def visit(tree, offset):\n",
    "        if isinstance(tree, Tree):\n",
    "            n_tokens = 0\n",
    "            for subtree in tree:\n",
    "                n_tokens += visit(subtree, offset + n_tokens)\n",
    "            symb = tree.label()\n",
    "            j = rule2feat[symb]\n",
    "            rule_feats[offset:offset+n_tokens, j] = 1\n",
    "            return n_tokens\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    visit(tree, 0)\n",
    "    seq = expr\n",
    "    lex_index = index\n",
    "    w_feats = rule_feats\n",
    "\n",
    "\n",
    "    ch_len = len(seq)\n",
    "    w_len  = w_feats.shape[0]\n",
    "    n_feats = w_feats.shape[1]\n",
    "    ch_feats = np.zeros((ch_len, n_feats))\n",
    "\n",
    "    for i_w in range(w_len):\n",
    "        f_w  = w_feats[i_w,...]\n",
    "        i_ch,len_ch = lex_index[i_w]\n",
    "        f_ch = np.tile(f_w, len_ch).reshape((len_ch, n_feats))\n",
    "        ch_feats[i_ch:i_ch+len_ch,:] = f_ch\n",
    "    return [ch_feats,nt_symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = test_sequences[0]\n",
    "a = generate_parsetree(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = a[0]\n",
    "names = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractfeature(intermiediate, inputdata):  \n",
    "    tree = intermiediate[0]\n",
    "    names = intermiediate[1]\n",
    "    seq= inputdata\n",
    "    pad_char='~'\n",
    "    w_size = 30\n",
    "    step_size = 5\n",
    "    if w_size == -1:\n",
    "        w_size = max(len(S) for S in sequences)\n",
    "    X = []\n",
    "    y = []\n",
    "    prov_index = {}\n",
    "\n",
    "    steps = range(step_size, len(seq), step_size)\n",
    "    if len(steps) == 0:\n",
    "        steps = [len(seq)-1]\n",
    "    for i in steps:\n",
    "        end = i\n",
    "        start = max(0, i-w_size)\n",
    "        s = seq[start:end]\n",
    "        assert len(s) <= w_size\n",
    "        x = s.rjust(w_size, pad_char)\n",
    "        assert len(x) == w_size\n",
    "        X.append(x)\n",
    "        y.append(seq[i])\n",
    "        prov_index[x] = (start, end)\n",
    "\n",
    "    featuress = []\n",
    "    for idx, name in enumerate(names):\n",
    "        features = []\n",
    "        for key, value in prov_index.items(): \n",
    "            start, end = value\n",
    "            feature = tree[start:end,idx]\n",
    "            L = end - start\n",
    "            if L < 30:\n",
    "                tmp = np.zeros((30))\n",
    "                tmp[-L:] = feature\n",
    "                feature = tmp\n",
    "            features.append(feature)\n",
    "        feat_name = 'F_' + name.symbol()\n",
    "        featuress.append(features)\n",
    "    return np.array(featuress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 54, 30)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = extractfeature(a,test_sequences[0])\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = test_sequences[0]\n",
    "pad_char='~'\n",
    "w_size = 30\n",
    "step_size = 5\n",
    "n_test_tuples = 128\n",
    "if w_size == -1:\n",
    "    w_size = max(len(S) for S in sequences)\n",
    "X = []\n",
    "y = []\n",
    "prov_index = {}\n",
    "\n",
    "steps = range(step_size, len(seq), step_size)\n",
    "if len(steps) == 0:\n",
    "    steps = [len(seq)-1]\n",
    "for i in steps:\n",
    "    end = i\n",
    "    start = max(0, i-w_size)\n",
    "    s = seq[start:end]\n",
    "    assert len(s) <= w_size\n",
    "    x = s.rjust(w_size, pad_char)\n",
    "    assert len(x) == w_size\n",
    "    X.append(x)\n",
    "    y.append(seq[i])\n",
    "    prov_index[x] = (i_seq, start, end)\n",
    "    if len(X) > n_test_tuples:\n",
    "        break\n",
    "\n",
    "test_from = X\n",
    "test_to = y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2int = {' ': 72, '$': 23, \"'\": 12, '(': 68, ')': 8, '*': 71, '+': 33, ',': 24, '-': 26, '.': 2,\n",
    " '/': 5, '0': 22, '1': 6, '2': 43, '3': 38, '4': 14, '5': 58, '6': 66, '7': 18, '8': 55, '9': 69,\n",
    " '<': 52, '=': 57, '>': 60, 'A': 1, 'B': 47, 'C': 9, 'D': 40, 'E': 62, 'F': 59, 'G': 21, 'H': 32,\n",
    " 'I': 70, 'J': 56, 'L': 15, 'M': 50, 'N': 63, 'O': 7, 'P': 31, 'R': 27, 'S': 39, 'T': 51, 'U': 25,\n",
    " 'V': 29, 'W': 49, 'Y': 20, 'a': 61, 'b': 64, 'c': 44, 'd': 37, 'e': 16, 'f': 54, 'g': 11, 'h': 28,\n",
    " 'i': 67, 'j': 17, 'k': 46, 'l': 3, 'm': 13, 'n': 42, 'o': 65, 'p': 41, 'q': 34, 'r': 10, 's': 19, \n",
    "'t': 36, 'u': 48, 'v': 53, 'w': 30, 'x': 4, 'y': 45, 'z': 35, '~': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoDimEncoders:\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_encoded(seqs, seqs2=None, cust_char2int=None):\n",
    "\n",
    "        if cust_char2int is None:\n",
    "            all_chars = reduce(lambda chars,seq : set(chars) | set(seq), seqs)\n",
    "            if seqs2 is not None:\n",
    "                all_chars |= reduce(lambda chars,seq : set(chars) | set(seq),\n",
    "                                    seqs2)\n",
    "            all_chars = sorted(all_chars)\n",
    "\n",
    "            char2int =  {c:i for i, c in enumerate(all_chars)}\n",
    "        else:\n",
    "            char2int = cust_char2int\n",
    "\n",
    "        encoded_seqs = [[char2int[x] for x in seq] for seq in seqs]\n",
    "        if seqs2 is None:\n",
    "            return encoded_seqs, char2int\n",
    "\n",
    "        encoded_seqs2 = [[char2int[x] for x in seq] for seq in seqs2]\n",
    "        return encoded_seqs, char2int, encoded_seqs2\n",
    "\n",
    "    @staticmethod\n",
    "    def encoded_to_bin_tensor(enc_seqs, char2int, enc_seqs2=None,\n",
    "                                start_at_min=False):\n",
    "\n",
    "        min_x = min(char2int.values()) if start_at_min else 0\n",
    "        n_chars = max(char2int.values()) - min_x + 1\n",
    "\n",
    "        seq_len = len(enc_seqs[0])\n",
    "\n",
    "        X = np.zeros((len(enc_seqs), seq_len, n_chars), dtype=np.int)\n",
    "        for i, enc_seq in enumerate(enc_seqs):\n",
    "            for j, x in enumerate(enc_seq):\n",
    "                k = x - min_x\n",
    "                X[i, j, k] = 1\n",
    "\n",
    "        if enc_seqs2 is None:\n",
    "            return X\n",
    "\n",
    "        X2 = np.zeros((len(enc_seqs2), seq_len, n_chars), dtype=np.int)\n",
    "        for i, enc_seq in enumerate(enc_seqs2):\n",
    "            for j, x in enumerate(enc_seq):\n",
    "                k = x - min_x\n",
    "                X2[i, j, k] = 1\n",
    "\n",
    "        return X, X2\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_bin_tensor(seqs, seqs2=None, cust_char2int=None):\n",
    "        encoding_out = TwoDimEncoders.raw_to_encoded(seqs, seqs2, cust_char2int)\n",
    "        if len(encoding_out) == 2:\n",
    "            encoded_seqs, char2int = encoding_out\n",
    "            encoded_seqs2 = None\n",
    "        else:\n",
    "            encoded_seqs, char2int, encoded_seqs2 = encoding_out\n",
    "\n",
    "        bin_tensors = TwoDimEncoders.encoded_to_bin_tensor(\n",
    "            encoded_seqs, char2int, encoded_seqs2)\n",
    "\n",
    "        if type(bin_tensors) is not tuple:\n",
    "            return bin_tensors, char2int\n",
    "\n",
    "        out = bin_tensors + (char2int,)\n",
    "        return out\n",
    "\n",
    "X_test, char2int = TwoDimEncoders.raw_to_bin_tensor(test_from, cust_char2int=char2int)\n",
    "    \n",
    "\n",
    "class OneDimEncoders:\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_encoded(seq, seq2=None, cust_char2int=None):\n",
    "\n",
    "        if cust_char2int is None:\n",
    "            all_chars = set(seq)\n",
    "            if seq2 is not None:\n",
    "                all_chars |= set(seq2)\n",
    "            all_chars = sorted(all_chars)\n",
    "\n",
    "            char2int =  {c:i for i, c in enumerate(all_chars)}\n",
    "        else:\n",
    "            char2int = cust_char2int\n",
    "\n",
    "        encoded_seq = [char2int[x] for x in seq]\n",
    "        if seq2 is None:\n",
    "            return encoded_seq, char2int\n",
    "\n",
    "        encoded_seq2 = [char2int[x] for x in seq2]\n",
    "        return encoded_seq, char2int, encoded_seq2\n",
    "\n",
    "    @staticmethod\n",
    "    def encoded_to_bin_tensor(enc_seq, char2int, enc_seq2=None,\n",
    "                                start_at_min=False, add_dim=False):\n",
    "\n",
    "        min_x = min(char2int.values()) if start_at_min else 0\n",
    "        n_chars = max(char2int.values()) - min_x + 1\n",
    "\n",
    "        X = np.zeros((len(enc_seq), n_chars), dtype=np.int)\n",
    "        for i, x in enumerate(enc_seq):\n",
    "            j = x - min_x\n",
    "            X[i, j] = 1\n",
    "\n",
    "        if add_dim:\n",
    "            X = X[:,np.newaxis,:]\n",
    "\n",
    "        if enc_seq2 is None:\n",
    "            return X\n",
    "\n",
    "        X2 = np.zeros((len(enc_seq2), n_chars), dtype=np.int)\n",
    "        for i, x in enumerate(enc_seq2):\n",
    "            j = x - min_x\n",
    "            X2[i, j] = 1\n",
    "\n",
    "        if add_dim:\n",
    "            X2 = X2[:,np.newaxis,:]\n",
    "\n",
    "        return X, X2\n",
    "\n",
    "    @staticmethod\n",
    "    def raw_to_bin_tensor(seq, seq2=None, cust_char2int=None, add_dim=False):\n",
    "        encoding_out = OneDimEncoders.raw_to_encoded(seq, seq2, cust_char2int)\n",
    "        if len(encoding_out) == 2:\n",
    "            encoded_seq, char2int = encoding_out\n",
    "            encoded_seq2 = None\n",
    "        else:\n",
    "            encoded_seq, char2int, encoded_seq2 = encoding_out\n",
    "\n",
    "        bin_tensors = OneDimEncoders.encoded_to_bin_tensor(\n",
    "            encoded_seq, char2int, encoded_seq2, add_dim=add_dim)\n",
    "\n",
    "        if type(bin_tensors) is not tuple:\n",
    "            return bin_tensors, char2int\n",
    "\n",
    "        out = bin_tensors + (char2int,)\n",
    "        return out\n",
    "\n",
    "    \n",
    "y_test, char2int = OneDimEncoders.raw_to_bin_tensor(test_to, cust_char2int=char2int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 30, 73)\n",
      "(54, 73)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f9d834f5be0>"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "newmodel = load_model(\"track_history/models-03-2.78.hdf5\")\n",
    "newmodel._make_predict_function()\n",
    "layer = [0]\n",
    "outputs = [newmodel.layers[l].output for l in layer]\n",
    "newspymodel = Model(inputs = newmodel.input, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 30, 128)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspymodel.predict(X_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 30, 73)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
